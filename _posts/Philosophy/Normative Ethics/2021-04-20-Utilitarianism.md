---
title: Why Utilitarianism should be rejected
layout: article
author: JayMoss
sidebar:
  nav: org
---
Here are the primary features of classic utilitarianism from the _Stanford Encyclopedia of Philosophy_ [entry on consequentialism](https://plato.stanford.edu/entries/consequentialism/) (as of [April 2021](http://web.archive.org/web/20210408110252/https://plato.stanford.edu/entries/consequentialism/)):

-   Consequentialism = whether an act is morally right depends only on _consequences_ (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act).
-   Evaluative Consequentialism = moral rightness depends only on the _value_ of the consequences (as opposed to non-evaluative features of the consequences).
-   Actual Consequentialism = whether an act is morally right depends only on the _actual_ consequences (as opposed to foreseen, foreseeable, intended, or likely consequences).
-   Direct Consequentialism = whether an act is morally right depends only on the consequences of _that act itself_ (as opposed to the consequences of the agent’s motive, of a rule or practice that covers other acts of the same kind, and so on).
-   Maximizing Consequentialism = moral rightness depends only on which consequences are _best_ (as opposed to merely satisfactory or an improvement over the status quo).
-   Aggregative Consequentialism = which consequences are best is some function of the values of _parts_ of those consequences (as opposed to rankings of whole worlds or sets of consequences).
-   Total Consequentialism = moral rightness depends only on the _total_ net good in the consequences (as opposed to the average net good per person).
-   Universal Consequentialism = moral rightness depends on the consequences for _all_ people or sentient beings (as opposed to only the individual agent, members of the individual’s society, present people, or any other limited group).
-   Equal Consideration = in determining moral rightness, benefits to one person matter _just as much_ as similar benefits to any other person (as opposed to putting more weight on the worse or worst off).
-   Agent-neutrality = whether some consequences are better than others does not depend on whether the consequences are evaluated from the perspective of the agent (as opposed to an observer).
-   Hedonism = the value of the consequences depends only on the _pleasures_ and _pains_ in the consequences (as opposed to other supposed goods, such as freedom, knowledge, life, and so on).

When I refer to "utilitarianism", I will be referring to this narrow definition. Utilitarianism, like most consequentialist moral theories, can be broken down into two broad components: (1) a theory of goodness, and (2) a theory of how the goodness of the outcome of an act relates to that act's rightness. Utilitarianism says that (1) goodness is constituted by the total summation of the pleasure over pain of all sentient creatures where each creature is given equal consideration, and (2) an act is right if and only if the actual consequences of that act have more goodness than any other alternative. Further, I believe that a theory of goodness can be broken down into two subcomponents: (1a) a theory of _individual goodness_, and (1b) a theory of how the goodness of individuals relates to the _collective_ goodness (e.g. using some sort of aggregation function). Utilitarianism says that (1a) individual goodness is constituted by the total amount of pleasure and pain for that individual, and (1b) the collective goodness is simply the total summation of every sentient creature's individual goodness. In this post, I will criticize each of these three components of utilitarianism. These criticisms, I believe, provide sufficient reason to reject utilitarianism.

### Individual goodness

One of the most unattractive features of utilitarianism is its theory of individual goodness: [hedonism](https://plato.stanford.edu/entries/hedonism/). Hedonism states that the value or goodness of our lives depends only on the pleasure and pain that we experience. I reject hedonism because it advocates performing the action that produces the most pleasure even when this conflicts with one's strongest goals and plans in life. For example, imagine that there exists a special drug such that consuming the drug for the rest of one's life was the best way to maximize the expected amount of pleasure one would experience. However, anyone who takes the drug becomes severely addicted and is rendered unable to pursue any of their other goals or plans in life; the rest of their life is spent in either inebriation or acquiring resources necessary to gather additional drugs to restore their inebriated state. Now imagine someone is given the opportunity to chose whether to consume the drug (also assume that their decision has no significant impact on other people, to avoid complications due to moral considerations).

It seems to me that this individual _could_ have more reason to decline to consume the drug, assuming he had other goals that he considered more important than the pleasure that would be produced by this drug, e.g. goals related to his career, projects, hobbies, etc. (I say "could" rather than "would" because presumably there _could_ be circumstances where an individual actually had more reason to consume the drug). However, this doesn't quite make sense under hedonism. Hedonism states that the best life is the one with more pleasure over pain, and presumably we have most reason (absent moral considerations) to make our lives go best. Thus, if hedonism were true, it could not be the case that the individual could have more reason to refrain from taking the drug since, by hypothesis, taking this drug would produce the highest expected amount of pleasure and (therefore) good for that person. For this reason, I believe a proper theory of individual goodness should be some sort of refined desire theory, where goodness for an agent is constituted by the satisfaction of that agent's fully informed and fully rational desires. More specifically, I believe I proper theory of individual goodness should be based on what an agent has _reason_ to desire, where an agent's reasons ([as I've argued here](https://reasonwithoutrestraint.com/reasons-internalism-an-argument-from-theoretical-reasons/)) are the rational extension of an agent's motivational set (motivational set includes elements such as desires, values, goals, plans, normative judgments, etc.).

### Aggregation Function

Now, let's assume we have a perfect theory of individual goodness. Let's consider the aggregation function that utilitarianism endorses, i.e. let's see what utilitarianism says about the relationship between (a) the goodness of _each_ of the lives of the individual members of a group and (b) the overall goodness of the group as a whole. Following SEP, the aggregation function used by utilitarianism states that the relationship between individual goodness and collective goodness has the following components: Aggregative Consequentialism, Total Consequentialism, Universal Consequentialism, Equal Consideration, and Agent-neutrality. I reject utilitarianism's aggregation function as I reject Total Consequentialism, Equal Consideration and Agent-neutrality. I will first consider Total Consequentialism.

Total Consequentialism states moral rightness depends only on the _total_ net good in the consequences (as opposed to the average net good per person). There are at least two objections to consequentialist theories that identify goodness with the summation of the individual goodness of each member in a group.

-   The first objection involves the possibility of [utility monsters](https://en.wikipedia.org/wiki/Utility_monster). Utility monsters are beings that produce far more value (according on one's preferred theory of individual goodness, etc. narrow hedonism, preference hedonism, desire-satisfaction, etc.) for each unit of resources they consume than anyone else. For example, if there is a utility monster, then it would be best to give it all of our food even if it results in our starvation. Intuitively, this seems incorrect; it would be extremely bad if we all starved even if it produced an extreme amount of value in another being.
-   A second objection is the [Repugnant Conclusion](https://plato.stanford.edu/entries/repugnant-conclusion/) presented by Derick Parfit. The basic idea is that, if we accept Total Consequentialism, then "For any possible population of at least ten billion people, all with a very high quality of life, there must be some much larger imaginable population whose existence, if other things are equal, would be better even though its members have lives that are barely worth living". In other words, a large number of individuals whose lives are only barely valuable can have more total goodness than a small number of individuals whose lives are significantly good. Again, intuitively, it seems that this kind of world would not be better than our own. Another reason to reject total consequentialism is that it treats all utility gains of a given amount as equivalent, although it seems that utility gains for those who are significantly worse-off are better than identical utility gains for those who are significantly better-off. For these reasons, _total_ consequentialism should be rejected.

The other two components of utilitarianism's aggregation function that I believe should be rejected are Equal Consideration and Agent-neutrality. Consider the following reasons to reject these components of utilitarianism.

-   Agents have moral permission to assign special intrinsic priority to particular agents. To see why, imagine a scenario where an agent can save the lives of several innocent persons by amputating one of his arms. It seems to me that while undergoing such a sacrifice might be admirable, it is certainly not morally _obligatory_. That is, an agent would not be morally _wrong_ (i.e. blameworthy) if they refused to undergo this sacrifice.
-   Agents are morally permitted to assign special priority to the goodness of their loved ones. For example, if a mother has to chose between saving her children versus a larger number of unrelated children (assume each of the unrelated children can be expected to produce just as much good as each of the mother's children), it seems that the mother has _permission_ to chose her own children rather than sacrificing her children. In other words, the mother is not morally _wrong_ or _blameworthy_ if she places special priority on her children. If my intuitions in these examples are reliable, then we should abandon the Equal Consideration and Agent-neutrality components of utilitarianism. Agents should have moral permission to ascribe to either themselves or their loved ones a special, finite weight to their goodness when evaluating the rightness of their actions.
-   Agent-neutrality rules out agent-relative constraints. Without agent-relative constraints, an agent would be obligated to kill one innocent person to save the lives of a greater number of innocent persons. For example, consider the [Transplant thought experiment](https://plato.stanford.edu/entries/consequentialism/#ConWhaRigRelRul) which is meant to be an objection to classic utilitarianism. Imagine that a doctor has five injured patients in a hospital who have each been stabbed in a different vital organ. Also, imagine that the doctor has the opportunity to kill a random healthy patient and distribute his organs to the sick patients to save the five. If we assume that each of the patients can be expected to produce the same amount of goodness in the future, then, on agent-neutral utilitarianism, the doctor would be morally _obligated_ to kill the healthy patient to save the five injured patients. This seems completely incorrect. The doctor is not morally obligated to kill the healthy patient in this case. In fact, it doesn't even seem that he is morally _permitted_ to kill the healthy patient. However, the only way to account for this intuition is to introduce agent-relative considerations in the theory of value. If we use an agent-neutral theory of value, then we examine the goodness produced by actions from an impartial observer's perspective. In that case, the world with less people dying is clearly superior to the world with more people dying. However, on an agent-relative theory of value, we examine the goodness produced by an action from the perspective of the agent who performs the action. With agent-relativity, we can say that, from the doctor's perspective, it is worse to kill the healthy patient because it is worse that _he_ kill an innocent patient than it is that someone else kills five innocents.

### Relationship between collective goodness and moral rightness

Now, let's assume we have a perfect theory of individual goodness and a perfect aggregation function, which provides us with a perfect way to measure collective goodness. Let's see how collective goodness relates to the rightness of actions. Following SEP, the following components of utilitarianism indicate how collective goodness relates to moral rightness: Actual Consequentialism, Direct Consequentialism and Maximizing Consequentialism. It seems to me that the adoption of actual and maximizing consequentialism has highly unintuitive implications.

1.  Firstly, it seems like foreseeable or intended consequences should be relevant instead of the actual consequences of an action. E.g. if someone attempts to murder an innocent person for an unacceptable reason (e.g. say, because they hated the race of the victim), this action is morally _wrong_ (i.e. the intended killer is morally _blameworthy_) regardless of whether their attempt was successful. On the other hand, if a driver accidentally kills a pedestrian because the pedestrian recklessly ran before his car (imagine that the driver didn't intend to injure anyone, nor could he reasonably foresee that the pedestrian would run into the world, nor was the driver negligent in any manner), then it doesn't seem that the driver is morally wrong, i.e. he doesn't seem blameworthy. So the actual consequences should be the basis for the moral assessment of an action. Rather, it should either be the intended or foreseeable consequences, as these consequences can inform us of the agent's choices, which is crucially relevant in assessing his character.
2.  Secondly, it seems like maximizing consequentialism is far [too demanding](https://philpapers.org/browse/demandingness-of-consequentialism). For the vast majority of agents, to maximize the goodness in the world would involve the agent undergoing huge sacrifices on themselves (so long as the agent does not ascribe to himself an unreasonably large priority). For example, imagine that an agent can kill themselves to save N other lives. Also, let N be greater than whatever additional weight the agent is free to ascribe to the goodness of his life. Let's say that the agent weighs the goodness of his life at 10 times anyone else and N=20. Assume the goodness to be produced by each person involved in this scenario is the same. In this scenario, under maximizing consequentialism, the agent would be morally required to kill himself as that would maximize goodness, but presumably this is not right. Another problem with maximizing consequentialism is that it does not allow for [_supererogatory_ actions](https://plato.stanford.edu/entries/supererogation/). That is, it doesn't allow for the existence of actions that are morally praiseworthy but which are not morally obligatory, but this seems wrong. It seems like it is possible for an agent to go above and beyond the requirements of morality, but this is not possible under maximizing consequentialism. For example, intuitively to spend all of one's discretionary income on charity would be morally _praiseworthy_, but not morally _obligatory_. Maximizing utilitarianism would rule out this class of actions entirely.

### Conclusion

To recap, I said that a consequentialist theory can be broken down into three components: (1) a theory of individual goodness (e.g. hedonism vs desire-theory), and (2) a theory of how the goodness of individuals relates to the collective goodness (e.g. summation vs average utilitarianism), and (3) a theory of how collective goodness relates to the rightness of actions (e.g. maximizing vs satisficing). I argued that utilitarianism has hugely unattractive implications for each of these components. In the future, [I'll make a post](https://reasonwithoutrestraint.com/why-im-not-a-consequentialist/) explaining why (1) consequentialist can avoid all of these issues that plague utilitarianism, and (2) despite this, why I'm not a consequentialist.