Readings:
- Consequentializing
	- Goals
		- Intro to consequentialism vs deontology. 
		- Included acts in the outcomes
		- Some forms of consequentialism:
			- Structure
				- Objective vs Subjective: what constitutes "right". Does the rightness of an action depend on its consequences, or expected/forseable consequences.
				- Decision Theory: what constitutes "ought". when acting, ought an agent decide what is right (e.g. what act, rule, set of motives has the best consequences) and then do that, or ought they act according to something else.
				- Consequences of what: act, rules, general motives
			- ??? (not sure of w name)
				- Maximizing vs Satisficing
				- Agent-neutral vs Agent-relative
				- Time-neutral vs Time-relative
			- Aggregation
				- Aggregative vs non-aggregative
				- Average, Summation, Pareto
			- Value
				- Constituitive vs Causal consequences
				- Forms of Value
	- SEP paper
	1 - [x] Robert Nozick, "Moral Constraints and the State" 
	https://spot.colorado.edu/~heathwoo/readings/nozick.pdf
	2 - [ ] Drier, "Structures of Normative Theories" (1993) 
	https://www.brown.edu/academics/philosophy/sites/brown.edu.academics.philosophy/files/uploads/StructuresOfNormativeTheories_0.pdf
	- [ ] Jennie Louise, "Relativity of Value and the Consequentialist Umbrella" (2004)
	3	- [ ] Portmore, "Consequentializing moral theories" (2007) 
		http://www.public.asu.edu/~dportmor/Consequentializing_Moral_Theories.pdf
		- [ ] Mark Schroeder, "Teleology, Agent-Relative Value, and ‘Good’" (2007) 
		https://www.jstor.org/stable/10.1086/511662
	- [ ] Portmore, "Consequentializing" (2009)
		- [ ] Peterson, "A Royal Road to Consequentialism?" (2010)
		https://pure.tue.nl/ws/files/3046038/Metis232152.pdf
	4 - [x] Brown, "Consequentialize This" (2011)
	https://www.research.ed.ac.uk/portal/files/12473535/BROWN_C_Consequentialize_This.pdf
	5 - [x] Drier, "In defense of consequentialization" (2011) 
	https://www.brown.edu/academics/philosophy/sites/brown.edu.academics.philosophy/files/uploads/Dreier%20In%20Defense%20of%20Consequentizling.pdf
	6 	- [ ] Schroeder, "You Don't Have to Do What's Best!" (2011)
		https://philarchive.org/archive/SCHYDH
		- [ ] Hurley, "Consequentializing and Deontologizing" (2013)
		https://www.academia.edu/13806543/Consequentializing_and_Deontologizing_Clogging_the_Consequentialist_Vacuum
		- [ ] Tenenbaum, "The Perils of Earnest Consequentializing" (2014)
		http://www.sergiotenenbaum.org/papers.html
	7	- [ ] Portmore, "Replies to Gert, Hurley, and Tenenbaum" (2014)
		https://philarchive.org/archive/PORPOCv1
	8 - [x] Schroeder, "Consequentializing and Its Consequences" (2017) 
	https://philarchive.org/archive/SCHCAI-11v1
	9 - [ ] Sauer, "The cost of consequentialization" (2019)
	https://onlinelibrary.wiley.com/doi/full/10.1111/meta.12347
- Aggregation
	- Does collective goodness even make sense?
		- [x] Robert Nozick, "Moral Constraints and the State" 
		- [ ] Christine Korsgaard, "The Relational Nature of the Good"
	- Repugnant Solution
		- https://plato.stanford.edu/entries/repugnant-conclusion/
		- originally found in Parfit's *Reasons and Person*
		- [ ] Torbjorn Tannsjo, "Why We Ought to Accept the Repugnant Conclusion." (2002)	
		- [ ] Thomas Søbirk Petersen, "On the Repugnance of the Repugnant Conclusion." (2006) 
		- [ ] Michael Huemer, "In Defence of Repugnance." (2008)
		- [ ] Derek Parfit, "Can We Avoid the Repugnant Conclusion?" (2016)
		- [ ] Ruth Chang, "Parity, Imprecise Comparability, and the Repugnant Conclusion." (2016)
		- [ ] James Fanciullo, "Imprecise Lexical Superiority and the (Slightly Less) Repugnant Conclusion." (2019)
	- Arguments for/against aggregation generally
		- [ ] Alastair Norcross, "Great Harms From Small Benefits Grow: How Death Can Be Outweighed by Headaches." (1998)
		- [ ] Alastair Norcross, "Two Dogmas of Deontology: Aggregation, Rights, and the Separateness of Persons" (2009)
		- [ ] Theron Pummer, "Intuitions About Large Number Cases." (2013)
		- [ ] Alex Voorhoeve, "How Should We Aggregate Competing Claims?" (2014)
		- [ ] Alex Voorhoeve, "Why Sore Throats Don't Aggregate Against a Life, but Arms Do." (2015)
		- [ ] Alex Voorhoeve "Balancing Small Against Large Burdens." (2018) 
	- Hard Numbers Skepticism (numbers never matter, even when harms are similar)
		- [x] Taurek, John Should the numbers count (1977)
		- [ ] Otsuka, Michael. "Skepticism about saving the greater number." (2004)
		- [ ] Timmermann, Jens. "The Individualist lottery: how people count, but not their numbers" (2004)
		- [ ] Kamm, Frances. 2005. "Aggregation and two moral methods."
		- [ ] Lawlor, Rob. "Taurek, numbers, and probabilities." (2006)
		- [ ] Meyer, Kirsten. "How to be consistent without saving the greater number." (2006)
		- [ ] Hsieh, Nien-Hê, Alan Strudler and David Wasserman. "Pairwise Comparison and Numbers Skepticism" (2007)
		- [ ] Willenken, Tim. "Deontic cycling and the structure of commonsense morality" (2012)
		- [ ] Yishai Cohen, "Don’t Count on Taurek: Vindicating the Case for the Numbers Counting" (2014)
	- Whether aggregation is consistent with a principle that says our duties are determined by each indivdual's claims seperately (e.g. in Scanon's contractualism; i.e. numbers matter when the harms are similar but not necessarily in other cases)
		- Defense:
			- Parfit, Derek. 1978. "Innumerate ethics." Philosophy and Public Affairs 7: 285–301
			- Scanlon, T.M. 1999. *What we owe to each other*
			- Kumar, Rahul. 2001. "Contractualism on saving the many."
			- Hirose, Iwao. 2001. "Saving the greater number without combining claims." Analysis 61: 341–342.
			- Hirose, Iwao. 2013. "Aggregation and the separateness of persons." Utilitas 25: 182–205.
			- Raz, Joseph. 2003. "Numbers, with and without contractualism."
			- Hsieh, Nien-Hê, Alan Strudler and David Wasserman. 2006. "The numbers problem."
		- Critiques:
			- Wasserman, David and Alan Strudler. "Can a nonconsequentialist count lives?" (2003) 
			- Otsuka, Michael. "Scanlon and the claims of the many versus the one."
			- Otsuka, Michael. "Saving lives, moral theory, and the claims of individuals." (2006)
			- Liao, Matthew S. Who is afraid of numbers? (2008),
			- Doggett, Tyler. What is wrong with Kamm and Scanlon’s arguments against Taurek (2009)

- Objections to utilitarianism/consequentialism
	- Demandingness: agent-relative vs agent-neutral
	- Time-neutral vs time-relative
	- Maximizing vs satisficing vs relational (relational = no moral obgliations, just ranking of states of affairs)
	- Aggregation vs non-aggregative
- Justified uses of force
- Deontology
	- Kant
	- Agent-centered
	- Patient-centered
	- Natural Law
	- Natural Rights
- Contractualism
	- Rawsls
	- Scanlon
		- What we Owe To Each Other 
		- Moral Dimensions
	- Darwall
		- The Second Person Standpoint
	- Southwood
		- "Moral Contractualism"
		- Contractualism and the Foundations of Morality
- Contractarianism
	- *Morals by Agreement*, Gauthier
- Kantian Contractualism - Parfit

This is to flesh out my substantive ethical theory, to specify specifically which actions are right and wrong in a way that can be characterized without reference to normative terms.

- Constructivism
	- Reasons are contingent, subjective constructions.
	- Moral discussions committed to reasons for all.
	- Moral discussions committed to methodological objectivity.
	- Moral obligations exist only for those contingently engaged in moral discussions.
	- Moral truth = socially constructed.
- Moral principles
	- Regulating societal approval/disapproval
- Legal principles
	- Regulating coercion
	- More disagreement -> more libertarian principles. Why?

- Goodness/Badness
	- Reason for an individual to promote/desire
	- Individual Instantaneous Goodness
	- Lifetime Goodness
	- Collective Goodness
- Rightness/Wrongness
	- Reason for a society to approve/praise/disapprove/resent
	- Wrongness requires something bad for someone
	- Large increases justify small deficits   
- Political Principles
	- Egalitarian Libertarian
	- Social Ownership
- Political Views
	- Abortion: Radical Pro-choice
	- Education: Robust Subsidization
	- Healthcare: Robust Subsidization
	- Gay Marriage: Mandated Legalized
	- Crimally Justice: Radical Rehabilitation
	- Welfare: Non-dsygenic
	- Affirmative Action: Ineffective
	- Immigration: Skilled, Required labor only
	- Discrimination: Legal in some circumstances

## Types of moral arguments

1. Reflective Equilibrium - Consistency and Intuitiveness
2. Linguistic - what we mean with ethical terms
3. Common commitments - rationality, universalization (e.g. from Kant)

To argue that a particular act X is wrong/right in a particular circumstance C, there are a few possible arguments:

- Appeal to some fundamental principle that provides a determinate answer to the rightness/wrongness of X in circumstance C. See below for arguments in favor of fundamental principles. Of course, this is assuming that the fundamental moral principles form determinate answers, which need not be the case. This also assumes that there are fundamental moral principles, which need not be the case.
- Appeal to the obvious rightness/wrongness of an analogous act X' in an analogous circumstance C'. This theory can remain agnostic to the truth/weight of any fundamental moral principles (if any such principles even exist). This strategy is committed only to the fact that there clearly is a constraint on which *considerations* are morally relevant (regardless of whether and how those considerations can be reduced to certain principles). Considerations such as happiness, desires, autonomy, responsibility, etc. are potentially relevant moral considerations (i.e. humans clearly use these considerations in ordinary moral thinking; whether they really should be used and/or whether some of these are only instrumentally valuable is a further question). But there are clearly considerations that don't count as morally relevant. This style of argument works if it can be shown that the differences between C and C' are definitely not potentially relevant moral considerations.

To argue for certain fundamental principles:

- Reflective Equilibrium: Argue that these principles best explain, or cohere with, our considered moral judgments. The problem here is that the best principles that explain our moral judgments might not be nonconflicting, i.e. the considerations that explain our considered moral judgments might not reduce to the same principles (which means it might not provide determinate answers in all circumstances), and/or different people might have fundamentally different irreconcilable considered moral judgments.
- Foundational: Build these principles from the ground up, either from a metaethical theory, or pure rationality, or pure consistency, or from minimal constraints on any coherent moral theory. This may be too ambitious.

## The Good

- Badness only occurs when there is conflict. Even though things might not be very good without conflict.
- Force only justified if other force is prevented. Even though things might not be very good if this weren't the case.

--> self-defense from blameworthy agent -> self-defense from non-blameworthy agent -> self-defense by harming unrelated agent.
--> radical egoism (everyone promote my interests) -> egoism -> 
--> All interests are not commensurate, e.g. there is no N such that losing a limb = N pinches. How to determine the layers of harm? Ask an individual if they themselves consider the harms to be commensurate, i.e. would they lose a limb in favor of N backrubs? If no, then these cannot be weighted for/against each other across persons.

### Individual Goodness

Concerns our reasons for desires/intentions

Only one constraint that I'm convinced of for now: in order for something to be good or bad for someone, that person has to be alive. Possibilties for individual goodness:

- Experience Based
	- Quantitative hedonism (sensory experience): 
		- What if someone doesn't value this intrinsically? 
	- Qualitative Hedonism (sensory experience): 
		- But how to compare different pleasures?
		- Mill's test seems unmotivated.
	- Preference hedonism (attitudinal pleasure): 
		- Preferred states of consciousness
		- Enjoyment
	- General Problems
		- What if someone's preferred state of consciousness is based on false beliefs? E.g. A thinks P is true which makes him happy, but P is false.
		- What if someone values something else intrinsically?
		- Should we prohibit fully informed decisions of adults if they choose something that lowers their happiness?
		- How to compare pleasure and pain? Not an objection per se, but defeats the alleged virtue of simplicity.
- Preference Based
	- Unrestricted preference satisfaction: 
		- People can have uninformed, irrational preferences
	- Fully informed, rational aims/preferences:
	- Success theory: Satisfaction of preferences about one's own life, i.e. preferences about features that are introspectively discernable
	- General Problems
		- If someone's preference is satisfied without their knowledge, how is that good for them?
- Other
	- Objective list theory / Pluralism: knowledge, friendship, etc.
		- Seems unmotivated.
		- What unites them?
- General Issues
	- Why assume its telelogical? 
		- Oftentimes to say X is good, we mean we have reason to promote X
		- But not always: e.g. loyalty is good =/= we have reason to promote loyalty, i.e. betraying a friend to promote loyalty is not to value loyalty.
	- Why assume we can reduce to a single utility, that can be represented by some number.
		- Lexical Orderings? E.g. no amount of small pleasures can outweigh torture.
	- How to quantify pleasure/preferences into a numerical unit?
	- What if pleasure/desires are sadistic? Should it be desert-adjusted?
	- What if someone doesn't deserve the pleasure/desire satisfaction? Double desert-adjusted?
- Best approach
	- Ignore ideas about "maximizing" or "increasing" or "promoting" some end good.
	- Rather, experiences are ranked according to whether people endorse those states.
	- There may be states that cannot be said to be strictly better than another because they are incomparable, e.g. if you have two options A and B, and regardless of the one you pick you will be happy for your choice. 

### Collective Goodness:

For aggregating across time and aggregating across time

- Interpersonal comparisons
	- How to measure the strength of one pleasure/preference over someone else's?
	- E.g. A prefers/takes pleasure in X whereas B prefers/takes pleasure in not-X
- Maximizing the sum of utility. 
	- Adding more people is somehow better. Who could have reason to bring in new people?
	- Repugnant conclusion: If everyone in S1 has X well-being and everyone in S2 has Y well-being (where X >>> Y), then S2 is better than S1 so longer as the number of people in S2 is large enough. Some examples that are against leftist reasoning:
		- Denying minorities certain rights in a bigoted society
		- Abortion must be outlawed
		- Hate speech against minorities
	- Utility Monster: one being produces far more pleasure than another, or far stronger preferences.
- Maximizing the average. 
	- Having children with below average utility makes the world a worse place.
	- Utility monster.
- Minimizing pain:
	- Killing people painlessly would be good.
	- Imagine inflicting small amount of pain to produce a large amount of pleasure.
- Other options:
	- Pareto optimality.
	- Principles that would be preferred by everyone if they didn't know where they would fall within the distribution.
	- Small diminishes to someone with high well-being for a large increase to someone with low well-being.
- Best approach
	- Ignore ideas about "maximizing" or "increasing" or "promoting" some end good.
	- Rather, states are ranked according to whether people prefer living in that world.
	- There may be states that cannot be said to be strictly better than another because they are incomparable, e.g. if people have different preferences. E.g. A would prefer living in a world with a higher average but lower floor, whereas B prefers living in a world with lower average but higher floor. 

## The Right - Fundamental moral principles 

---

In order for an action to be wrong, it must be bad for someone, i.e. limiting in their fully rational aims (their "interests") in some way.

However, interests are not the only considerations relevant for morality. Moral rightness doesn't just reduce to non-moral goodness. The "weight" of an interest (even if fully rational) is not sufficient to generate moral duties. 
- Responsiblity moderates the legitimacy of the interests. E.g. we have less of a duty to alleviate burdens from someone who was fully responsible for their condition.
- Morally illegitimate interests. E.g. We don't have to respect someone's desire to not see gays in public.
- Degenerate Interests don't have the same weight as non-degenerate interests. e.g. We don't have to respect desires to harm others

Thus, moral rightness does not reduce to non-moral goodness. However, it's an open question as to whether there can be a notion of moral goodness seperate from non-moral goodness, to which morality reduces. This can possibly reduce morality to consequentialism.

---

There are fundamentally different kinds of aims. Cannot be reduced to one another, neither in their significance to the person nor in their relevance for moral decisions (e.g. coercion cannot be justified by compensating preferred state of consciousness):

1. Preferred states of consciousness
2. Resource acquisition
3. Bodily autonomy

Limitations on (3) can only be justified by preventing other limitations on (3). Limitations on (2) can be justified to prevent limitations on (2) or (3). Limitations on (1) can be justified by preventing limitations on (1), (2) or (3).

Argument for deontic restrictions ("side constraints")
1. [Premise] Moral Reason: A is morally obligated to X only if A has no reason to reject a principle that requires him to X
2. [Premise] Reasons: A has reason to reject a principle that requires him to subjugate himself as a means to maximize the impersonal good.
3. [Premise] Symmetry: if A is not morally obligated to X, then others are not morally permitted to coerce A into doing X.
4. [from 1 and 2] Moral Perogative: A is morally permitted to refuse to subjugate himself as a means to maximize the impersonal good.
5. [from 3 and 4] Moral restrictions: Other people are not morally permitted to coerce A into subjugation as a means to maximize the impersonal good.

### Against utilitarianism

Problems:
1. How to quantify the good?
	- In ordinary circumstances (not philosophical thought experiments), there is a balance of certain people being harmed on both sides (consider restrictions on free speech). How do you know that the harms from one side outweigh the other?
2. What aggregation function for collective well-being?
	- See earlier issues
3. Why assume that the right reduces to the good?
	- The good is all we care about. Sure, but that says nothing about morality. Plenty of normative realms are not just matters of promoting the good (epistemology, ettiquette, etc.), so why assume that's the case for morality? E.g. epistemology is not consequentialist.
	- Metaethical: morality just is social rationality, and rationality is promoting the good. Is that what morality is? What is "social rationality".
4. Why does everyone have reason to accept this? Why would X have reason to make significant sacrifices to himself to improve the overall state? 
	- It seems individuals don't have reason to accept a system that would subjugate them to impersonal optimality. To use intuitions to show this: if utilitarianism were true, then individual agents would be morally required to subjugate *themselves* to impersonal optimality (not just that *others* would be morally required to do this). But it does not seem that individuals have reason to do this. Because agents have no duty to do what they lack reason to do, agents do not have a moral duty to maximize the good

## Consequentialism



Main objections to utilitarianism:
Structure: how the ranking of outcomes relates to the rightness of acts
- Maximizing: can be met with satisficing consequentialism
Collective Goodness: ranking outcomes for groups
- Demandingness: can be met with agent-relative consequentialism
- Aggregation: can be met with non-aggregative consequentialism
Individual Goodness: ranking outcomes for individuals
- Hedonism: can be met with non-hedonistic utilitarianism (e.g. preference utilitarianism)
- Commensurability: can be met with pluralistic utilitarianism

Why to reject consequentialism:
- Against the consequentialist thesis:
	- There is not always a way to definitively determine which world is better than another. I.e. reject that "better than" is a total order. Some worlds cannot be compared, e.g. when some are worse off and others are better off, or when worlds have different amounts of people.
	- In cases where A is definitively better than B, there could still be moral constraints preventing us from instantiating A over B:
		- So the moral does not reduce to goodness.
		- Objection: we can include those moral constraints into the goodness evaluation.
		- But I think people shouldn't violate those constraints even if it would reduce the number of those violations (i.e. even if it would be better).
		- Objection: we can make those moral constraints time- and agent-indexed.
		- But I think there's a notion of objective goodness that is not time- or agent-indexed. In that I think people ought to attempt to instantiate this notion of goodness, conditional on them obeying the moral constraints. This notion of objective goodness is important because: there's a notion of goodness that I think should regulate the behavior of an omnipotent, impartial observer who is deciding on how to instantiate certain realities into existence. THIS notion of goodness, however, is not what grounds our moral duties, and THIS notion of goodness does not include any side constraints.
		- As for subjective goodness (i.e. what is good-for an agent), these can be agent-indexed (though not indexed). However, these do not determine one's moral duties.
		- Perhaps it is a combination of subjective goodness and objective goodness that determines our moral duties. So rightness really does just reduce to goodness. Some problems:
			- What are the relative priorities of objective and subjective goodness? Clearly, this cannot be explained by goodness. One needs to introduce deontological reasoning. However, one can object that this requires deontological reasoning, but it doesn't influence the output. The output is still expressable in consequentialist terms. However, there is no sense in which the outcomes can be ranked in a way that I agree are maximizing goodness. Because I don't have some third conception of goodness that weighs objective and subjective goodness.
			- There are some features releavant to moral duties that I don't believe relate to objective or subjective goodness. I.e. responsibility, the content of one's interests, desert, etc. These are features that I would not want an omnipotent, impartial observer to take into account when instantiating the world, which I do think ought to be the moral constraints on behavior.
	- One cannot avoid consequentializing by denying that all moral theories can be consequentialized. Clearly any moral theory CAN be consequentialized. However, one can avoid BEING a consequentialIST:
		- One can posit a person theory of goodness and a personal moral theory and argue that these diverge. Some (e.g. Drier) argue that our concept of goodness is not seperate from our concept of ought-to-be-pursued, thus these two theories cannot really come apart. For something to be good just is to say that it ought to be pursued. So any moral constraints on what we ought to do can be interpreted as beliefs about goodness. However, as shown above, there is a sense of goodness that is seperate from anything that determines moral constraints.  
		- One can emphasize the differences that deontology and consequentialist give to the REASONS for our moral duties. Deontology might point to contractual considerations or respect for autonomy, whereas consequentialism points to well-being. It could be argued that deontological considerations can be expressed in the form of consequences, e.g. one is really considered with minimizing agent-relative and time-relative violations of autonomy or contracts, but this does not actually ring true of the reasoning of actual deontologists. Consequentialists begin by evaluating states of affairs, i.e. ranking realities according to some preferred metric. Deontologists do not do this, e.g. they start from the relationships we have with one another. It is phenomonologically implausible to suggest we are starting from a bunch of agent-indexed, time-indexed ranking of realities and then reasoning from there.
		- Of course a consequentializer can emphasize that for any deontologist, his moral theory can be converted to a consequentialist theory + a particular theory of the good. This is true, however, the point for the deontologist is to deny that theory of the good. Thus, the deontology/consequentialist divide may not be most appropriate as properties of moral THEORIES (i.e. if theories are just defined by their deontic output); rather, they should be seen as (1) properties of THEORISTS and (2) properties about the reasons/justifications/rationale for the theories. 
- Epistemic Problems
	- Comparing and quantifying goodness across individuals seems implausible
- Reject collective goodness in general.
	- Collective goodness is clearly related to individual goodness. Specifically, collective goodness supervenes on individual goodness.
	- Does it even make sense to talk about collective goodness?
	- Goodness relates to what we have reason to do.
	- It makes sense to talk about what is good for X, because it relates to what X has reason to desire.
	- But what does it mean to talk about goodness simpliciter? How can there be reasons simpliciter to desire something? 
	- Specific individuals might have reason to prefer (collective) certain states of affairs over others (e.g. say, if they were to choose what kind of distribution to be randomly placed into, or what kind of life to continually live). But this can differ by individual depending on their level of riskiness.
	- Further, there might be certain situations that everyone has reason to prefer, but only in trivial circumstances, e.g. a policy that makes everyone else better off. But consequentialism purports to provide more than this, it is supposed to explain how sometimes harming certain people is offset by benefiting others.
	- Instead of looking torwards evaluations of collective states of affairs, we seek principles that constrain our behavior without reference to any notions of final goodness. 
- Reject completeness: 
	- Agents have multiple optional spaces of consequences to produce
	- Satisficing vs maximizing consequentialism
	- This is not JUST satisficing, because agents are permitted to be sub-optimal so long as they don't cause extreme suffering.
	- Further, agents have PERMISSION (not OBLIGATION) to value themselves. E.g. if they chose to save themselves instead of 2 othes, or if they chose to save 2 others instead of themselves, both  are permissible and neither is blameworthy.
- Reject agent-neutrality: 
	- Agent-relative vs neutral consequentialism
	- Agents can over-value themselves
	- Agents are not obligated to do henious actions, even if it leads to a positive net utility (even after taking into account their over-evaluation of themselves). E.g. if raping an innocent women leads to fewer rapes in the future, we are not obligated to rape her. A further question is whether this would be a good thing.
- Reject aggregation: 
	- many small pleasures =/= one terrible experience
	- Applies over people and over time
	- Seperateness of persons. Two bads distributed over people is not twice as bad as an equal bad over one person, since there is no individual who experiences the two bads.
- Reject commensurability
	- Not all types of goodness can be compared to one another
	- There is no number line or to rank them, what does a number line even mean?
- Reject hedononism
	- There are other goods, i.e. desire satisfaction.
	- We need to adjust for certain kinds of dessert (at least for a theory of morality, not necessarily for a theory of goodness)
		- Certain people deserve pleasure more than others, e.g. good people.
		- Degenerate interests. Certain pleasures are inherently worse than others. Namely, external pleasures/preferences, i.e. preferences about the wellbeing of others. E.g. pleasures in the suffering of others, selfish pleasure - a desire to have more than others.
		- Responsibility. People who willingly take actions to reduce those pleasures can be given less weight than those whose pleasures are reduced due to no fault of their own.
		- Intentions. An intentional act might be equally as good as an unintentional act (e.g. imagine they have the same consequences) but the intentional act is subject to moral evaluation whereas the latter is not.
			- Intentional vs accidental act: e.g. voluntary and nonvoluntary act with the same consequences
			- Action vs Inaction: e.g. killing someone versus refraining from doing something that leads to someone's death.
		- These probably are not relevant to a theory of goodness, i.e. there is nothing necessarily better or worse about degenerate interests being treated similarly as non-degenerate interests, but they are relevant to a theory of morality. Which shows that the rightness of actions does not always reduce to the goodness of its resulting state of affairs.
		- Argument: the future goodness of a group supervenes on the future goodness of each individual. The future goodness of each individual is independent of their previous actions and of the content of their desires; it depends only on their future desire satisfaction. Therefore, the goodness of a group supervenes on the future desire satisfaction. However, the rightness/wrongness of an action does not supervene on the future desire satisfaction of the individuals; it also depends on the previous actions of the individuals and the content of their desires. 
		- The wrongness of an action does relate to goodness in some way. In order for something to be morally wrong, it needs to be bad for someone.

All systems are aimed at reaching some state of affairs. The question is what constitutes the value of those states of affairs. E.g. deontology can be considered something that "A should not X" means "A should reduce the promotion of A doing X in the particular instance that A acts".

Notes:
- This should be seen as criticisms of utilitarianism rather than consequentialism
- The consequentialism/deontology divide perhaps should be abandoned since it is unclear if there really is a distinction. More interesting is the agent-neutral/agent-relative divide.

Features of consequentialism:

From most necessary for consequentialism to least necessary:

- Requirements:
	- Supervenience: The moral rightness of acts supervenes on the non-indexed goodness of the resulting outcomes. 
		- I actually agree that goodness matters to morality in the following senses:
			1. In order for an act to be wrong, it needs to make at least one person worse-off.
			2. The rightness of an act supervenes on the *indexed* goodness of each individual. I.e. An act that results in A having +10 utils and B having -4 utils is no more or less right than another act with the identical result. However, this does not mean these would be equivalent to A having -4 utils and B having +10 utils. The non-indexed goodness is identical, but indexing might matter (i.e. if A is more deserving)
	- Completeness: for all possible states of affairs and region of time, there is a complete order. "Complete" means for all possibilities, x,y: U(x) >= U(y) or U(y) >= U(x). "Order" means transitive and reflexive. Must be additive in a very general sense: there is a function f that takes as input a given space-time region of the universe (a state of affairs across a period of time) and returns some integer representing its overall value. This determines moral rightness/wrongness.
		- Consequentialism can deny saying the only right action is the one with the best consequences.
			- One could be satisficing. I.e. one's actions are right if they are good enough. 
			- One could reject the dichotonomy altogether. There are no right/wrong actions, just better/worse.
		- My rejections:
			- Agents are permitted to cause suffering so long as they don't cause extreme suffering.
			- Agents are PERMITTED (but not necessarily obligated) to over-value themselves. There is a space for permissible actions (e.g. sacrificing oneself for the sake of others or not doing so are both permissible).
	- If the below conditions are rejected, the system basically becomes a way for you to endorse whatever states of affairs you prefer living in. This can be consequentialist in a broad sense, but does not meet the maximizing spirit of consequentialism. There are two reasons for this:
		1. Because there is effectively no measure, it doesn't make much sense to say "more utility" is guiding the decisions. Rather, one just endorses the kind of world that they prefer living in. Appealing to utility does no work here.
		2. This is a rather egoistic interpretation, which goes against the spirit of conseqentialism. Someone might say the world that they prefer to live in is one where everyone maximizes his pleasure. But that doesn't mean it satisfies the spirit of consequentialism.
- Agent-neutrality and temporal-neutrality:
	- R is an agent-neutral reason for A to X (i.e. A has reason to X *because* of fact R) iff: R is characterized without reference to A. This focuses on the general form or principle of R, not just the particular token of the reason. E.g. the general form of the reason in "A has reason to help his friend because it would make his friend happy" can be either "A has reason to make his friends happy" (agent-relative) or "A has reason to make others happy" (agent-neutral). 
	- The state of affairs to be promoted can *always* be characterized without reference to the particular agent doing the acting. We can say X is better than Y simpliciter, not X is better than Y *for* Bob or Jane. This is not to say that we ought not do different things in particular cases. We can, but the goal is always towards the same ideal state of affairs (e.g. maybe Bob doing X and Jane doing not-X is the best way to maximize utility). E.g. a world with 1 bad act > world with 2 bad acts. Thus, we should achieve the former between the two. This means A must perform 1 bad act if it prevents 2 bad acts. The only this can be avoided by defining good state of affairs to be relative to the acting agent, but this cannot be done. 
	- E.g. the moral obligation for an agent does not give a special status to that agent. This seems off to me. Agents can value themselves slightly more than others.
	- E.g. the moral obligation is towards a non-indexed state of affairs, not an agent-indexed action. E.g. A has reason to rape if it reduces overall rape. Not sure about this.  
	- Similar remarks can be made about certain times: the state of affairs to be promoted can be characterized without reference to the time that the agent does the acting.
	- Egoism rejects this, but egoism shouldn't be understood as consequentialism which is focused on maximizing *The* Good. *The* Good assumes there is one good to be maximized, not many different goods relative to different agents.
- Aggregative/Atomism/Maximizing: 
	- Value of the whole = sum of value of the parts. Small utilities build up over large utilities. Contrast this atomistic consequentialism with a hollistic form of consequentialism, which might, e.g., look at distributions as relevant.
	- This entails reasons which are agent/time-neutral, or at least not *strictly* agent/time-relative (see above). E.g. if a reason is strictly agent-relative, i.e. concerning the occurence of a particular agent performing an action, then there is no sense in a duty concerned with aggregating over everyone's actions and summing utilities up.
	- Aggregative across people. E.g. assuming U(A) >= U(B) >= 0 where A and B are persons, U(A+B) > max(U(A), U(B)). Assuming N number of people with identical individual utility, U strictly increases as N increases. I reject this.
	- Aggregative across time. E.g. assuming U(X) >= U(Y) >= where X and Y are time slices of the world. U(X+Y) > max(U(X), U(Y)). Assuming t timespan of a particular region with identical instantaneous utility, U strictly increases with t. I reject this.
- Commensurability/Comparability: 
	- Must be promotional of a single quantifiable utility. All values (e.g. pleasure, suffering, pain, freedom, beauty, knowledge, virtuous actions, virtuous intentions, rights, etc.) can be reduced to single comparable utility units.
	- Also, its possible to compare different person's pleasures, preferences, etc.
	- E.g. not just a hierarchy of possible states of affairs (even a deontologist would agree with that). The difference is that consequentialist rankings must be determined by the existence of a certain amount of a good. Some constant good in the universe that we appeal to to measure utility. Needed in order to quantify goodness on a number line.
- Hedonism: 
	- Pleasure is the single value (this does not include "suffering" or "pain" except as a lack of value).
	- It doesn't matter who gets the pleasure and whether the pleasure is warranted. 
	- See double-desire adjusted attitudinal hedonomism
	- Contrast this with pluralism.

Not all of these features are necessary for all consequentialism. All are accepted by hedonistic utilitarianism, but not accepted by all consequentialists. Prerequisite list:

Consequentialism < Teleology (teleology = requirement for consequentialism)
Consequentialism < Completeness (Completeness = requirement for consequentialism)
Consequentialism <~ Agent/Time-neutrality (Neutrality required if consequentialism is focused on maximizing *The* Good, rather than the Good *for* a particular agent)
Aggregation < Agent/Time-neutrality
Hedonism < Aggregation
Hedonism < Commensurability

From SEP entry on consequentialism

- Consequentialism = whether an act is morally right depends only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act).
- Evaluative Consequentialism = moral rightness depends only on the value of the consequences (as opposed to non-evaluative features of the consequences).
- System
	- Actual Consequentialism = whether an act is morally right depends only on the actual consequences (as opposed to foreseen, foreseeable, intended, or likely consequences).
	- Direct Consequentialism = whether an act is morally right depends only on the consequences of that act itself (as opposed to the consequences of the agent's motive, of a rule or practice that covers other acts of the same kind, and so on).
	- Maximizing Consequentialism = moral rightness depends only on which consequences are best (as opposed to merely satisfactory or an improvement over the status quo).
- Value
	- Hedonism = the value of the consequences depends only on the pleasures and pains in the consequences (as opposed to other supposed goods, such as freedom, knowledge, life, and so on).
- Aggregation Function
	- Aggregative Consequentialism = which consequences are best is some function of the values of parts of those consequences (as opposed to rankings of whole worlds or sets of consequences).
	- Total Consequentialism = moral rightness depends only on the total net good in the consequences (as opposed to the average net good per person).
- Neutrality
	- Universal Consequentialism = moral rightness depends on the consequences for all people or sentient beings (as opposed to only the individual agent, members of the individual's society, present people, or any other limited group).
	- Equal Consideration = in determining moral rightness, benefits to one person matter just as much as similar benefits to any other person (= all who count count equally).
	- Agent-neutrality = whether some consequences are better than others does not depend on whether the consequences are evaluated from the perspective of the agent (as opposed to an observer).

Ideas about recipricol morality:
- Our moral duties are all based on contingent relationships. I.e. friends have a duty to be loyal to each other. Romantic partners might have a duty to be monogamous. We have obligations not to lie, keep promise, maintain contracts, etc. because these reflect an initial commitment that implicitly entered into such a relationship.
- To enter into a relationship is to accept constraints on your individual goodness in certain contexts in favor of others. E.g. romantic partners accept constraining their freedom to pursue other mates. This provides them the reason/duty to not cheat, and provides us with a reason to blame them if they do.
- The importance of morality is regulating the reasons we have to blame others, not necessarily what people have reason to follow. So the question is what reasons do we have to blame others who do not belong to a neat relationship. What is the "moral relationship" that most people belong to? We can determine what relationships people accept by asking what constraints they accept on their behavior. This probably varies from each pair of individuals. E.g. people might accept more constraints on their behavior to help their countryman than to help other nations.
- Our moral reasons depend on the constraints that others accept. 
	- For those that accept no constraints (i.e. egoists), we have no moral criticism to their behavior. I.e. we cannot blame him. THat's because he is not a moral agent; he's more like an alien, intelligent animal or enemy in war. But we do have very good reason to coerce and manipulate them into whatever we want, and they have no moral objection to our behavior. By their own lights, our actions would be morally permissible. Thus, our behavior can be justified to them. Because our behavior can be justified to *them*, they are justified *full stop*.
	- For those that accept minimum constraints (e.g. parties of different countries, who accept non-aggression and sovereignty), we only have moral criticism when they e.g. initiate aggression (more realistically, they probably also accept suffering minor constraints to get other nations out of dire situations). If they fail to suffer more constraints to help us, we have no moral objection. Of course, we might coerce them if we had power, but this would not be morally justified. This could not be justified to them.
	- For those that accept more robust constraints (e.g. friends, family, etc.) we can criticize their behavior insofar as they fail to constrain their behavior appropriately.
- General strategy for determining how we are morally permitted to act towards person X:
	1. Layout X's proposed relationships, i.e. the constraints he is willing to accept.
	2. We have reason to blame X whenever he violates such constraints.
	3. We are morally permitted to treat X in a way that doesn't violate his constraints.
	4. The constraints cannot involve aggregation (i.e. "I accept a constraint whereby I constrain my behavior when it maximizes happiness")
	5. The constraints must be general, not specific. I.e. "I accept reducing my happiness by 10 units to increases someone else's happiness by 100" or "I accept suppressing my goal in contexts C if I promised someone else that I would supress said goal in said consequences". They cannot be "I accept not initiating aggression" because the reason they accept not initiating aggression might be because of their fortunate circumstances, and it would be unfair to those in unfortunate circumstances to not be morally permitted to initiate aggression. 
		- I probably wouldn't want the constraints to be expressed in terms of happiness. So perhaps promises/contracts form the basis of everyone's constraints. Those would be the only ways to generally articulate a principle without using something immeasurable like utils.
	6. We need to know the constraints that people ACTUALLY adopt, not what they WOULD adopt if they were e.g. behind a veil of ignorance. Because we're concerned with their actual relationships and actual reasons. Behind the veil of ignorance, people are in a different sort of relationship with different reasons. One might say this allows for the advantaged to select principles that self-serve them, leaving the disadvantaged without permission to object, but this should be solved by making the principles general.
		- One problem: what if they say "I accept fully constraining myself and never harming anyone else if I am disadvantaged but I accept no constraints if I am advantaged".
- This might be a decent strategy for spelling out the duties under contractualism. In order to figure out what can be justified to others, we determine what those others take to be justified.
- This strategy can be useful in determining what is sufficient for moral permission, or, equivalently, what is necessary for moral constraints. That is, if B does not accept constraint C with regard to treatment of A, then A has permission to treat B without respect to C. But is it also necessary? If B *does* accept constraint C with regard to treatment of A, is A morally obligated to accept constraint C? This doesn't seem so. E.g. if B says "I will never do an action that hurts someone else's feelings", that doesn't mean A has a duty to not hurt B's feelings. 