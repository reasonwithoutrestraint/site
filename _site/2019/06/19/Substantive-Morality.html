<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Substantive morality | Your awesome title</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Substantive morality" />
<meta name="author" content="JayMoss" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is to flesh out my substantive ethical theory, to specify specifically which actions are right and wrong in a way that can be characterized without reference to normative terms." />
<meta property="og:description" content="This is to flesh out my substantive ethical theory, to specify specifically which actions are right and wrong in a way that can be characterized without reference to normative terms." />
<link rel="canonical" href="http://localhost:4000/2019/06/19/Substantive-Morality.html" />
<meta property="og:url" content="http://localhost:4000/2019/06/19/Substantive-Morality.html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-19T00:00:00-07:00" />
<script type="application/ld+json">
{"description":"This is to flesh out my substantive ethical theory, to specify specifically which actions are right and wrong in a way that can be characterized without reference to normative terms.","author":{"@type":"Person","name":"JayMoss"},"@type":"BlogPosting","url":"http://localhost:4000/2019/06/19/Substantive-Morality.html","headline":"Substantive morality","dateModified":"2019-06-19T00:00:00-07:00","datePublished":"2019-06-19T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/06/19/Substantive-Morality.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/blog.html">Blog</a><a class="page-link" href="/">Home</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h1>Substantive morality</h1>
<p>19 Jun 2019 - JayMoss</p>

<p>This is to flesh out my substantive ethical theory, to specify specifically which actions are right and wrong in a way that can be characterized without reference to normative terms.</p>

<ul>
  <li>Constructivism
    <ul>
      <li>Reasons are contingent, subjective constructions.</li>
      <li>Moral discussions committed to reasons for all.</li>
      <li>Moral discussions committed to methodological objectivity.</li>
      <li>Moral obligations exist only for those contingently engaged in moral discussions.</li>
      <li>Moral truth = socially constructed.</li>
    </ul>
  </li>
  <li>Moral principles
    <ul>
      <li>Regulating societal approval/disapproval</li>
    </ul>
  </li>
  <li>Legal principles
    <ul>
      <li>Regulating coercion</li>
      <li>More disagreement -&gt; more libertarian principles. Why?</li>
    </ul>
  </li>
  <li>Goodness/Badness
    <ul>
      <li>Reason for an individual to promote/desire</li>
      <li>Individual Instantaneous Goodness</li>
      <li>Lifetime Goodness</li>
      <li>Collective Goodness</li>
    </ul>
  </li>
  <li>Rightness/Wrongness
    <ul>
      <li>Reason for a society to approve/praise/disapprove/resent</li>
      <li>Wrongness requires something bad for someone</li>
      <li>Large increases justify small deficits</li>
    </ul>
  </li>
  <li>Political Principles
    <ul>
      <li>Egalitarian Libertarian</li>
      <li>Social Ownership</li>
    </ul>
  </li>
  <li>Political Views
    <ul>
      <li>Abortion: Radical Pro-choice</li>
      <li>Education: Robust Subsidization</li>
      <li>Healthcare: Robust Subsidization</li>
      <li>Gay Marriage: Mandated Legalized</li>
      <li>Crimally Justice: Radical Rehabilitation</li>
      <li>Welfare: Non-dsygenic</li>
      <li>Affirmative Action: Ineffective</li>
      <li>Immigration: Skilled, Required labor only</li>
      <li>Discrimination: Legal in some circumstances</li>
    </ul>
  </li>
</ul>

<h2 id="types-of-moral-arguments">Types of moral arguments</h2>

<p>To argue that a particular act X is wrong/right in a particular circumstance C, there are a few possible arguments:</p>

<ul>
  <li>Appeal to some fundamental principle that provides a determinate answer to the rightness/wrongness of X in circumstance C. See below for arguments in favor of fundamental principles. Of course, this is assuming that the fundamental moral principles form determinate answers, which need not be the case. This also assumes that there are fundamental moral principles, which need not be the case.</li>
  <li>Appeal to the obvious rightness/wrongness of an analogous act X’ in an analogous circumstance C’. This theory can remain agnostic to the truth/weight of any fundamental moral principles (if any such principles even exist). This strategy is committed only to the fact that there clearly is a constraint on which <em>considerations</em> are morally relevant (regardless of whether and how those considerations can be reduced to certain principles). Considerations such as happiness, desires, autonomy, responsibility, etc. are potentially relevant moral considerations (i.e. humans clearly use these considerations in ordinary moral thinking; whether they really should be used and/or whether some of these are only instrumentally valuable is a further question). But there are clearly considerations that don’t count as morally relevant. This style of argument works if it can be shown that the differences between C and C’ are definitely not potentially relevant moral considerations.</li>
</ul>

<p>To argue for certain fundamental principles:</p>

<ul>
  <li>Reflective Equilibrium: Argue that these principles best explain, or cohere with, our considered moral judgments. The problem here is that the best principles that explain our moral judgments might not be nonconflicting, i.e. the considerations that explain our considered moral judgments might not reduce to the same principles (which means it might not provide determinate answers in all circumstances), and/or different people might have fundamentally different irreconcilable considered moral judgments.</li>
  <li>Foundational: Build these principles from the ground up, either from a metaethical theory, or pure rationality, or pure consistency, or from minimal constraints on any coherent moral theory. This may be too ambitious.</li>
</ul>

<h2 id="the-good">The Good</h2>

<ul>
  <li>Badness only occurs when there is conflict. Even though things might not be very good without conflict.</li>
  <li>Force only justified if other force is prevented. Even though things might not be very good if this weren’t the case.</li>
</ul>

<p>–&gt; self-defense from blameworthy agent -&gt; self-defense from non-blameworthy agent -&gt; self-defense by harming unrelated agent.
–&gt; radical egoism (everyone promote my interests) -&gt; egoism -&gt; 
–&gt; All interests are not commensurate, e.g. there is no N such that losing a limb = N pinches. How to determine the layers of harm? Ask an individual if they themselves consider the harms to be commensurate, i.e. would they lose a limb in favor of N backrubs? If no, then these cannot be weighted for/against each other across persons.</p>

<h3 id="individual-goodness">Individual Goodness</h3>

<p>Concerns our reasons for desires/intentions</p>

<p>Only one constraint that I’m convinced of for now: in order for something to be good or bad for someone, that person has to be alive. Possibilties for individual goodness:</p>

<ul>
  <li>Experience Based
    <ul>
      <li>Quantitative hedonism (sensory experience):
        <ul>
          <li>What if someone doesn’t value this intrinsically?</li>
        </ul>
      </li>
      <li>Qualitative Hedonism (sensory experience):
        <ul>
          <li>But how to compare different pleasures?</li>
          <li>Mill’s test seems unmotivated.</li>
        </ul>
      </li>
      <li>Preference hedonism (attitudinal pleasure):
        <ul>
          <li>Preferred states of consciousness</li>
          <li>Enjoyment</li>
        </ul>
      </li>
      <li>General Problems
        <ul>
          <li>What if someone’s preferred state of consciousness is based on false beliefs? E.g. A thinks P is true which makes him happy, but P is false.</li>
          <li>What if someone values something else intrinsically?</li>
          <li>Should we prohibit fully informed decisions of adults if they choose something that lowers their happiness?</li>
          <li>How to compare pleasure and pain? Not an objection per se, but defeats the alleged virtue of simplicity.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Preference Based
    <ul>
      <li>Unrestricted preference satisfaction:
        <ul>
          <li>People can have uninformed, irrational preferences</li>
        </ul>
      </li>
      <li>Fully informed, rational aims/preferences:</li>
      <li>Success theory: Satisfaction of preferences about one’s own life, i.e. preferences about features that are introspectively discernable</li>
      <li>General Problems
        <ul>
          <li>If someone’s preference is satisfied without their knowledge, how is that good for them?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Other
    <ul>
      <li>Objective list theory / Pluralism: knowledge, friendship, etc.
        <ul>
          <li>Seems unmotivated.</li>
          <li>What unites them?</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>General Issues
    <ul>
      <li>Why assume its telelogical?
        <ul>
          <li>Oftentimes to say X is good, we mean we have reason to promote X</li>
          <li>But not always: e.g. loyalty is good =/= we have reason to promote loyalty, i.e. betraying a friend to promote loyalty is not to value loyalty.</li>
        </ul>
      </li>
      <li>Why assume we can reduce to a single utility, that can be represented by some number.
        <ul>
          <li>Lexical Orderings? E.g. no amount of small pleasures can outweigh torture.</li>
        </ul>
      </li>
      <li>How to quantify pleasure/preferences into a numerical unit?</li>
      <li>What if pleasure/desires are sadistic? Should it be desert-adjusted?</li>
      <li>What if someone doesn’t deserve the pleasure/desire satisfaction? Double desert-adjusted?</li>
    </ul>
  </li>
  <li>Best approach
    <ul>
      <li>Ignore ideas about “maximizing” or “increasing” or “promoting” some end good.</li>
      <li>Rather, experiences are ranked according to whether people endorse those states.</li>
      <li>There may be states that cannot be said to be strictly better than another because they are incomparable, e.g. if you have two options A and B, and regardless of the one you pick you will be happy for your choice.</li>
    </ul>
  </li>
</ul>

<h3 id="collective-goodness">Collective Goodness:</h3>

<p>For aggregating across time and aggregating across time</p>

<ul>
  <li>Interpersonal comparisons
    <ul>
      <li>How to measure the strength of one pleasure/preference over someone else’s?</li>
      <li>E.g. A prefers/takes pleasure in X whereas B prefers/takes pleasure in not-X</li>
    </ul>
  </li>
  <li>Maximizing the sum of utility.
    <ul>
      <li>Adding more people is somehow better. Who could have reason to bring in new people?</li>
      <li>Repugnant conclusion: If everyone in S1 has X well-being and everyone in S2 has Y well-being (where X »&gt; Y), then S2 is better than S1 so longer as the number of people in S2 is large enough. Some examples that are against leftist reasoning:
        <ul>
          <li>Denying minorities certain rights in a bigoted society</li>
          <li>Abortion must be outlawed</li>
          <li>Hate speech against minorities</li>
        </ul>
      </li>
      <li>Utility Monster: one being produces far more pleasure than another, or far stronger preferences.</li>
    </ul>
  </li>
  <li>Maximizing the average.
    <ul>
      <li>Having children with below average utility makes the world a worse place.</li>
      <li>Utility monster.</li>
    </ul>
  </li>
  <li>Minimizing pain:
    <ul>
      <li>Killing people painlessly would be good.</li>
      <li>Imagine inflicting small amount of pain to produce a large amount of pleasure.</li>
    </ul>
  </li>
  <li>Other options:
    <ul>
      <li>Pareto optimality.</li>
      <li>Principles that would be preferred by everyone if they didn’t know where they would fall within the distribution.</li>
      <li>Small diminishes to someone with high well-being for a large increase to someone with low well-being.</li>
    </ul>
  </li>
  <li>Best approach
    <ul>
      <li>Ignore ideas about “maximizing” or “increasing” or “promoting” some end good.</li>
      <li>Rather, states are ranked according to whether people prefer living in that world.</li>
      <li>There may be states that cannot be said to be strictly better than another because they are incomparable, e.g. if people have different preferences. E.g. A would prefer living in a world with a higher average but lower floor, whereas B prefers living in a world with lower average but higher floor.</li>
    </ul>
  </li>
</ul>

<h2 id="the-right---fundamental-moral-principles">The Right - Fundamental moral principles</h2>

<p>In order for an action to be wrong, it must be bad for someone, i.e. limiting in their fully rational aims in some way.</p>

<p>There are fundamentally different kinds of aims. Cannot be reduced to one another, neither in their significance to the person nor in their relevance for moral decisions (e.g. coercion cannot be justified by compensating preferred state of consciousness):</p>

<ol>
  <li>Preferred states of consciousness</li>
  <li>Resource acquisition</li>
  <li>Bodily autonomy</li>
</ol>

<p>Limitations on (3) can only be justified by preventing other limitations on (3). Limitations on (2) can be justified to prevent limitations on (2) or (3). Limitations on (1) can be justified by preventing limitations on (1), (2) or (3).</p>

<p>Argument for deontic restrictions (“side constraints”)</p>
<ol>
  <li>[Premise] Moral Reason: A is morally obligated to X only if A has no reason to reject a principle that requires him to X</li>
  <li>[Premise] Reasons: A has reason to reject a principle that requires him to subjugate himself as a means to maximize the impersonal good.</li>
  <li>[Premise] Symmetry: if A is not morally obligated to X, then others are not morally permitted to coerce A into doing X.</li>
  <li>[from 1 and 2] Moral Perogative: A is morally permitted to refuse to subjugate himself as a means to maximize the impersonal good.</li>
  <li>[from 3 and 4] Moral restrictions: Other people are not morally permitted to coerce A into subjugation as a means to maximize the impersonal good.</li>
</ol>

<h3 id="against-utilitarianism">Against utilitarianism</h3>

<p>Problems:</p>
<ol>
  <li>How to quantify the good?
    <ul>
      <li>In ordinary circumstances (not philosophical thought experiments), there is a balance of certain people being harmed on both sides (consider restrictions on free speech). How do you know that the harms from one side outweigh the other?</li>
    </ul>
  </li>
  <li>What aggregation function for collective well-being?
    <ul>
      <li>See earlier issues</li>
    </ul>
  </li>
  <li>Why assume that the right reduces to the good?
    <ul>
      <li>The good is all we care about. Sure, but that says nothing about morality. Plenty of normative realms are not just matters of promoting the good (epistemology, ettiquette, etc.), so why assume that’s the case for morality? E.g. epistemology is not consequentialist.</li>
      <li>Metaethical: morality just is social rationality, and rationality is promoting the good. Is that what morality is?</li>
      <li>Intuition:
        <ul>
          <li>From an impartial perspective (e.g. behind a veil of ignorance, not using indexicals, etc. and only looking at goods of foundational value, e.g. happiness, preferences generally, etc. regardless of whether it is degenerate interests, distinguishing between deserved/predictable harms versus undeserved/unpredictable harms, etc.), I might agree that it is “better” to live in world where there were less innocent deaths. But that doesn’t commit me to thinking it is moral to kill an innocent to reduce deaths (even if there was a rule that reliably reduced the death efficiently by killing innocents). One might say that this is because I think killing innocents is worse than killing people dying due to desert or negligence. However, I don’t know why these features are relevant from an impartial perspective (i.e. behind the veil of ignorance, you only care about happiness generally, not whether its distributed toward those with the appropriate intentions). From an impartial perspective, there is no distinction between two worlds with identical amount of happiness, preference-satisfaction, etc. etc. where in one world the happiness is distributed to the virtuous but in another case it is distributed toward the vicious. From an impartial perspective, these are “equally” good, i.e. would be preferred equally by someone who didn’t know if they would be vicious or virtuous. However, I believe there is a moral distinction. So there is clearly a difference between good and moral.</li>
          <li>And if we are not speaking about our preferences from behind the veil of ignorance, then my preferences might be fairly egoistic. In this case, “good” might coincide with “moral”. But then what I count to be “good” would probably diverge greatly from what others think.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Why does everyone have reason to accept this? Why would X have reason to make significant sacrifices to himself to improve the overall state?
    <ul>
      <li>It seems individuals don’t have reason to accept a system that would subjugate them to impersonal optimality. To use intuitions to show this: if utilitarianism were true, then individual agents would be morally required to subjugate <em>themselves</em> to impersonal optimality (not just that <em>others</em> would be morally required to do this). But it does not seem that individuals have reason to do this. Because agents have no duty to do what they lack reason to do, agents do not have a moral duty to maximize the good</li>
    </ul>
  </li>
  <li>People morally obligated to sacrifice themselves for (1) the the life of another, if it is known that the other’s life would be better, and (2) the well-being of many others, if it is known that their well-being would exceed your own.</li>
</ol>

<h2 id="consequentialism">Consequentialism</h2>

<p>Features of consequentialism:</p>

<p>All systems are aimed at reaching some state of affairs. The question is what constitutes the value of those states of affairs. E.g. deontology can be considered something that “A should not X” means “A should reduce the promotion of A doing X in the particular instance that A acts”.</p>

<p>From most necessary for consequentialism to least necessary:</p>

<ul>
  <li>Requirements:
    <ul>
      <li>Moral right/wrongness reduces to goodness of the resulting states of affairs.</li>
      <li>Universal Teleology. All reasons are reasons to bring certain entities into existence or promote states of affairs.</li>
      <li>Completeness: for all possible states of affairs and region of time, there is a complete order. “Complete” means for all possibilities, x,y: U(x) &gt;= U(y) or U(y) &gt;= U(x). “Order” means transitive and reflexive. Must be additive in a very general sense: there is a function f that takes as input a given space-time region of the universe (a state of affairs across a period of time) and returns some integer representing its overall value.</li>
      <li>If the below conditions are rejected, the system basically becomes a way for you to endorse whatever states of affairs you prefer living in. This can be consequentialist in a broad sense, but does not meet the maximizing spirit of consequentialism. There are two reasons for this:
        <ol>
          <li>Because there is effectively no measure, it doesn’t make much sense to say “more utility” is guiding the decisions. Rather, one just endorses the kind of world that they prefer living in. Appealing to utility does no work here.</li>
          <li>This is a rather egoistic interpretation, which goes against the spirit of conseqentialism. Someone might say the world that they prefer to live in is one where everyone maximizes his pleasure. But that doesn’t mean it satisfies the spirit of consequentialism.</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>Agent-neutrality and temporal-neutrality:
    <ul>
      <li>R is an agent-neutral reason for A to X (i.e. A has reason to X <em>because</em> of fact R) iff: R is characterized without reference to A. This focuses on the general form or principle of R, not just the particular token of the reason. E.g. the general form of the reason in “A has reason to help his friend because it would make his friend happy” can be either “A has reason to make his friends happy” (agent-relative) or “A has reason to make others happy” (agent-neutral).</li>
      <li>The state of affairs to be promoted can <em>always</em> be characterized without reference to the particular agent doing the acting. We can say X is better than Y simpliciter, not X is better than Y <em>for</em> Bob or Jane. This is not to say that we ought not do different things in particular cases. We can, but the goal is always towards the same ideal state of affairs (e.g. maybe Bob doing X and Jane doing not-X is the best way to maximize utility). E.g. a world with 1 bad act &gt; world with 2 bad acts. Thus, we should achieve the former between the two. This means A must perform 1 bad act if it prevents 2 bad acts. The only this can be avoided by defining good state of affairs to be relative to the acting agent, but this cannot be done.</li>
      <li>E.g. the moral obligation for an agent does not give a special status to that agent. This seems off to me. Agents can value themselves slightly more than others.</li>
      <li>E.g. the moral obligation is towards a non-indexed state of affairs, not an agent-indexed action. E.g. A has reason to rape if it reduces overall rape. Not sure about this.</li>
      <li>Similar remarks can be made about certain times: the state of affairs to be promoted can be characterized without reference to the time that the agent does the acting.</li>
      <li>Egoism rejects this, but egoism shouldn’t be understood as consequentialism which is focused on maximizing <em>The</em> Good. <em>The</em> Good assumes there is one good to be maximized, not many different goods relative to different agents.</li>
    </ul>
  </li>
  <li>Aggregative/Atomism/Maximizing:
    <ul>
      <li>Value of the whole = sum of value of the parts. Small utilities build up over large utilities. Contrast this atomistic consequentialism with a hollistic form of consequentialism, which might, e.g., look at distributions as relevant.</li>
      <li>This entails reasons which are agent/time-neutral, or at least not <em>strictly</em> agent/time-relative (see above). E.g. if a reason is strictly agent-relative, i.e. concerning the occurence of a particular agent performing an action, then there is no sense in a duty concerned with aggregating over everyone’s actions and summing utilities up.</li>
      <li>Aggregative across people. E.g. assuming U(A) &gt;= U(B) &gt;= 0 where A and B are persons, U(A+B) &gt; max(U(A), U(B)). Assuming N number of people with identical individual utility, U strictly increases as N increases. I reject this.</li>
      <li>Aggregative across time. E.g. assuming U(X) &gt;= U(Y) &gt;= where X and Y are time slices of the world. U(X+Y) &gt; max(U(X), U(Y)). Assuming t timespan of a particular region with identical instantaneous utility, U strictly increases with t. I reject this.</li>
    </ul>
  </li>
  <li>Commensurability/Comparability:
    <ul>
      <li>Must be promotional of a single quantifiable utility. All values (e.g. pleasure, suffering, pain, freedom, beauty, knowledge, virtuous actions, virtuous intentions, rights, etc.) can be reduced to single comparable utility units.</li>
      <li>Also, its possible to compare different person’s pleasures, preferences, etc.</li>
      <li>E.g. not just a hierarchy of possible states of affairs (even a deontologist would agree with that). The difference is that consequentialist rankings must be determined by the existence of a certain amount of a good. Some constant good in the universe that we appeal to to measure utility. Needed in order to quantify goodness on a number line.</li>
    </ul>
  </li>
  <li>Hedonism:
    <ul>
      <li>Pleasure is the single value (this does not include “suffering” or “pain” except as a lack of value).</li>
      <li>Contrast this with pluralism.</li>
    </ul>
  </li>
</ul>

<p>Not all of these features are necessary for all consequentialism. All are accepted by hedonistic utilitarianism, but not accepted by all consequentialists. Prerequisite list:</p>

<p>Consequentialism &lt; Teleology (teleology = requirement for consequentialism)
Consequentialism &lt; Completeness (Completeness = requirement for consequentialism)
Consequentialism &lt;~ Agent/Time-neutrality (Neutrality required if consequentialism is focused on maximizing <em>The</em> Good, rather than the Good <em>for</em> a particular agent)
Aggregation &lt; Agent/Time-neutrality
Hedonism &lt; Aggregation
Hedonism &lt; Commensurability</p>

<p>From SEP entry on consequentialism</p>

<ul>
  <li>Consequentialism = whether an act is morally right depends only on consequences (as opposed to the circumstances or the intrinsic nature of the act or anything that happens before the act).</li>
  <li>System
    <ul>
      <li>Actual Consequentialism = whether an act is morally right depends only on the actual consequences (as opposed to foreseen, foreseeable, intended, or likely consequences).</li>
      <li>Direct Consequentialism = whether an act is morally right depends only on the consequences of that act itself (as opposed to the consequences of the agent’s motive, of a rule or practice that covers other acts of the same kind, and so on).</li>
      <li>Maximizing Consequentialism = moral rightness depends only on which consequences are best (as opposed to merely satisfactory or an improvement over the status quo).</li>
    </ul>
  </li>
  <li>Value
    <ul>
      <li>Evaluative Consequentialism = moral rightness depends only on the value of the consequences (as opposed to non-evaluative features of the consequences).</li>
      <li>Hedonism = the value of the consequences depends only on the pleasures and pains in the consequences (as opposed to other supposed goods, such as freedom, knowledge, life, and so on).</li>
    </ul>
  </li>
  <li>Aggregation Function
    <ul>
      <li>Aggregative Consequentialism = which consequences are best is some function of the values of parts of those consequences (as opposed to rankings of whole worlds or sets of consequences).</li>
      <li>Total Consequentialism = moral rightness depends only on the total net good in the consequences (as opposed to the average net good per person).</li>
    </ul>
  </li>
  <li>Neutrality
    <ul>
      <li>Universal Consequentialism = moral rightness depends on the consequences for all people or sentient beings (as opposed to only the individual agent, members of the individual’s society, present people, or any other limited group).</li>
      <li>Equal Consideration = in determining moral rightness, benefits to one person matter just as much as similar benefits to any other person (= all who count count equally).</li>
      <li>Agent-neutrality = whether some consequences are better than others does not depend on whether the consequences are evaluated from the perspective of the agent (as opposed to an observer).</li>
    </ul>
  </li>
</ul>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
