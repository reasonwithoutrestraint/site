# Substantive morality

- Constructivism
	- Reasons are contingent, subjective constructions.
	- Moral discussions committed to reasons for all.
	- Moral discussions committed to methodological objectivity.
	- Moral obligations exist only for those contingently engaged in moral discussions.
	- Moral truth = socially constructed.
- Moral principles
	- Regulating societal approval/disapproval
- Legal principles
	- Regulating coercion
	- More disagreement -> more libertarian principles. Why?

- Goodness/Badness
	- Reason for an individual to promote/desire
	- Individual Instantaneous Goodness
	- Lifetime Goodness
	- Collective Goodness
- Rightness/Wrongness
	- Reason for a society to approve/praise/disapprove/resent
	- Wrongness requires something bad for someone
	- Large increases justify small deficits   
- Political Principles
	- Egalitarian Libertarian
	- Social Ownership
- Political Views
	- Abortion: Radical Pro-choice
	- Education: Robust Subsidization
	- Healthcare: Robust Subsidization
	- Gay Marriage: Mandated Legalized
	- Crimally Justice: Radical Rehabilitation
	- Welfare: Non-dsygenic
	- Affirmative Action: Ineffective
	- Immigration: Skilled, Required labor only
	- Discrimination: Legal in some circumstances

## Types of moral arguments

To argue that a particular act X is wrong/right in a particular circumstance C, there are a few possible arguments:

- Appeal to some fundamental principle that provides a determinate answer to the rightness/wrongness of X in circumstance C. See below for arguments in favor of fundamental principles. Of course, this is assuming that the fundamental moral principles form determinate answers, which need not be the case. This also assumes that there are fundamental moral principles, which need not be the case.
- Appeal to the obvious rightness/wrongness of an analogous act X' in an analogous circumstance C'. This theory can remain agnostic to the truth/weight of any fundamental moral principles (if any such principles even exist). This strategy is committed only to the fact that there clearly is a constraint on which *considerations* are morally relevant (regardless of whether and how those considerations can be reduced to certain principles). Considerations such as happiness, desires, autonomy, responsibility, etc. are potentially relevant moral considerations (i.e. humans clearly use these considerations in ordinary moral thinking; whether they really should be used and/or whether some of these are only instrumentally valuable is a further question). But there are clearly considerations that don't count as morally relevant. This style of argument works if it can be shown that the differences between C and C' are definitely not potentially relevant moral considerations.

To argue for certain fundamental principles:

- Reflective Equilibrium: Argue that these principles best explain, or cohere with, our considered moral judgments. The problem here is that the best principles that explain our moral judgments might not be nonconflicting, i.e. the considerations that explain our considered moral judgments might not reduce to the same principles (which means it might not provide determinate answers in all circumstances), and/or different people might have fundamentally different irreconcilable considered moral judgments.
- Foundational: Build these principles from the ground up, either from a metaethical theory, or pure rationality, or pure consistency, or from minimal constraints on any coherent moral theory. This may be too ambitious.

## The Good

### Individual Instantaneous Goodness

Only one constraint that I'm convinced of for now: in order for something to be good or bad for someone, that person has to be alive. Possibilties for individual goodness:

- Narrow hedonism (sensory pleasure): states of consciousness marked by some distinctive feature
- Preference hedonism (attitudinal pleasure): preferred states of consciousness
- Unrestricted preference satisfaction: satisfaction of whatever preferences
- Success theory: satisfaction of preferences about one's own life, i.e. preferences about features that are introspectively discernable
- Objective list theory: knowledge, friendship, etc.
- Other constraints: S taking attitudinal pleasure in state of affairs P, only when S deserves that pleasure and P deserves to happen.

### Individual Lifetime Goodness

Faces similar problems as collective goodness. See below:

### Collective Goodness:

- *Summation*
- *Average*
- *Pareto optimality*
- Principles that would be preferred by everyone if they didn't know where they would fall within the distribution.
- Small diminishes to someone with high well-being for a large increase to someone with low well-being.

## The Right - Fundamental moral principles 

### Against utilitarianism
 
Three components of utilitarianism:
1. Measure of individual good: for utilitarianism, this is happiness
2. Measure of collective good: maximizing total individual well-being of all agents equally.
3. Consequentialism: the right reduces to the good in some way. 

Problems:
1. How is happiness defined? (don't have huge opposition) 
	- Pleasure receptors. This is bad.
	- States of enjoyment. Better 
	- Conscious states that are preferred. Better
	- Preferences/desires. Doesn't seem like happiness
2. What aggregation function for collective well-being?
	- Maximizing the sum of utility. Bad, because increasing people is always better. So it can be good to torture someone to increase marginal utility in X number of people, so long as X is large enough.  
	- Maximizing the average. Bad, because having children with below average utility makes the world a worse place.
	- It seems clear that world X is better than world Y, where every individual in world X has higher utility than in world Y (or they would have higher utility if placed in world Y). But if this is not true for _every_ individual, then I'm not sure.
	- Utility Monster: The gap in sentience between pigs and humans entails that it would be better for an innocent pig to die to save a human. But it doesn't seem that a comparable gap between a human and a superintelligent alien entails that its better for an innocent human to die to save the alien. Certainly, it doesn't seem like we would be morally justified in killing the human to save the alien. And especially the human wouldn't be morally obligated to kill himself. What if the alien mistakenly put himself in a position whereby either he or a human had to die?  
3. Why assume that the right reduces to the good?
	- The good is all we care about. Sure, but that says nothing about morality. Plenty of normative realms are not just matters of promoting the good (epistemology, ettiquette, etc.), so why assume that's the case for morality? E.g. epistemology is not consequentialist.
	- Metaethical: morality just is social rationality, and rationality is promoting the good. Is that what morality is?
	- Intuition: if we could prove that subjecting someone to a course of action maximized their good (even if good is defined in terms of preference satisfaction), is it right to force someone to do this for their own sake? Just because doing something promotes good doesn't mean that performing that action is good (particularly if it uses force). This scenario is possible in the following scenario: (1) person has ideal (more informed, higher order, more rational, etc.) preferences that would be satisfied by doing X, but (2) he doesn't willingly do X, and this must be because of less ideal preferences. It would be wrong to force this person to do X if (3) he also has preferences that he not be forced to do X in situations where (1) and (2) are true; but it would be okay to use force if he preferred to be forced to do X in these situations. Of course, (3) says nothing about whether performing X really is good (maybe it is, maybe it isn't); the fact is we can't use force.

### Other

My necessary conditions for an action to be morally wrong:

- It must be bad for someone. This is agnostic with respect to one's theory of goodness.
- It must be the right kind of badness. The following sources of badness don't count:
    - Someone arbitrarily opposed to an action, i.e. people disgusted with homosexuality

Separability of persons:
- It seems a person can never be morally required to accept a principle that causes a severe harm to compensate for some comparable benefit for another person or group of persons.
- Maybe that only applies when 

Applying FORCEFUL action F to agent A is morally prohibited, unless:

- A explicitly consents (probably does not count as forceful, but I'll include it here for completeness).
- A implicitly consents, while informed/rational
	- A implicitly suggests that they desire F (e.g. cues in romantic situations).
	- A doesn't desire F, but they engage in an informal agreement that clearly indicated that F would be done to them (honor cultures, gangs, etc.). A implicit agrees to F iff they perform a behavior X whereby:
	    - There is a clear standard that F is the consequence for X,
	    - Performing X is optional, and they are not coerced into doing X.
- A is too ignorant/irrational to decide whether to have F done, and we believe that doing F promote's A's interests (maybe this isn't sufficient, maybe we also need A to prefer to be coerced under the condition that they are irrational/uninformed; otherwise, this force may not be permissible).
> A is causally responsible for a gap in wellfare between himself and others; F is permissible insofar as it is the only reasonable method to diminish this gap. IMPORTANT: this is not just a matter of saying all actions must maximize our interests equally; you can favor your own interets; the point is, you cannot be causally responsible for diminishing someone else's interests
	- A directly impedes the interests of others (e.g. theft, assault, etc.)
	- A indirectly impedes the interests of others (e.g. monopolizing an industry and charges high prices)

## Other Topics

Topics:
- Striking somebody in revenge
- Do sentience gaps justify sacrifice


