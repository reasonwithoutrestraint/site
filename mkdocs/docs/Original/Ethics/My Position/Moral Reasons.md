# Moral Reasons

Where this lies in study of ethics

1. MetaEthics - semantics, psychology, epistemology, metaphysics of normativity and reasons generally.
2. Moral Reasons - The above applied to morality reasons specifically. E.g. the relationship between morality and normativity/rationality, between morality and motivation, etc.
3. Normative Ethics - Systems of broad substantive principles to settle a wide range of moral questions.
4. Applied Ethics - Specific principles to settle specific moral questions.

??? example "Readings"
	Derek Parfit
		- _Reasons and Persons_ (1984)
		- _On What Matters_ (2011)
		- "Justifiability to Each Person" (2003)
		- look up his discussion on "Kantian Contractualism"
	Thomas Scanlon
		- _What We Owe to Each Other_
		- "Contractualism and What We Owe to Each Other"
		- "Contractualism and Utilitarianism"
		- _Moral Dimensions: Permissibility, Meaning, Blame_
	T. Pogge, “What We Can Reasonably Reject” (2001)
	R. Kumar, "Reasonable reasons in contractualist moral argument" (2003)
	- Gilbert Harman, "Moral Relativism Defended"
	- Philippa Foot, "Morality as a System of Hypothetical Imperatives"
	- Essay: Nick Zangwill, "Externalist Moral Motivation"
	- Essay: David Brink, "Externalist Moral Realism"
	- David Gauthier, _Morals By Agreement_ (as in DGR)
	- Habermas,
	- Christine Kosgaard, "Kant's Formulation of Universal Law"

??? question "Some questions"
	- Can ethical truth in the contractualist sense be known a proiri or a posteriori, i.e. only after actual discussion with other people about what they have reason to reject?
	- Contractualism rejects that the only relevant consideration for moral reasoning is well-being. It includes anything that people have reason to care about. Some questions:
		1. Can this "cares" be reduced to a common good? It doesn't seem like it. It obviously wouldn't be well-being. Maybe something like goals.
		2. If yes, can we ignore referring to specific kinds of goals? Instead use a language that just refers to intrinsic goals generally without losing importance? Doesn't seem like it. E.g. imagine an agent had two goals: to be entertained and to provide food for his children. It you characterize this just as goal X and goal Y, you seem to lose just how much more important Y is compared to X. 
		3. If yes, can goals be compared numerically? Harms of qualitative differences don't seem comparable, e.g. physical versus emotional harm. Also, significant quantitative differences in harm don't seem comparable on a linear scale, e.g. being tortured versus a pinch can't be compared on some numerical scale such that N instances of a pinch matches 1 instance of torture. Some kinds of harms:
			- Sensation
				- Physical pain
				- Emotional pain
			- Liberty limitation
				- In ability, e.g. losing a limb, mobility, etc.
				- In opportunity, e.g. discrimination, oppression, etc.

## Moral Reasons

Expressive and Prescriptive

0. Standard Personal reasons - I satisfy my interests 
1. Speaker-personal reasons - everyone satisfy my interests
2. Interpersonal reasons - everyone their interests
3. Impartial interpersonal reasons - everyone satisfy everyone's interests
4. Reasons for action versus reasons for blame/shame

Like all normative domains, moral concerns what agents *ought* to do. And like all ought-statements, these ought claims are to be explained by the *reasons* they have for acting as such. So morality consists of a system of moral reasons, and agents who act immorally are not receptive to the moral reasons that apply to them. The task is to characterize what is distinctive of moral reasons, explaining how they differ from normative reasons in general and (possibly) how they differ from practical reasons in general. To judge that an action X is morally wrong implies some variation of the following ought claims: (1) agents ought not to intend X, (2) agents ought to feel guilt for intending X, and (3) society ought to impose social sanctions against intending X. Someone who invokes any of these ought-claims judges that there are reasons explaining why agents ought to act as claimed.

In addition to these normative implications of moral judgments, there are some non-normative implications (regarding motivation) as well: the agent is disposed to regulate his behavior as prescribed by the judgment, and the agent thinks others are *capable* of being motivated to regulate their behavior in accord with the norm. They are capable of being receptive to the reasons favoring the norm. E.g. while it might make sense for A to *dislike* B's preference for ice crean, it doesn't make sense for A to *disapprove* of B's preference for ice cream, but it would make sense if B regarded A as his dietary adviser. This is true even when B's actions might harm A. E.g. it doesn't make sense for A to disapprove of B's attack against him if B would be completely unreceptive (e.g. animal, child, evil person, aliens etc.), but it would make sense for someone who was receptive (i.e. normal rational agent).

These features can be exposed by considering the implicit commitments of moral judgments. Consider an implicit commitment of *assertions*: if someone says "X is P", we would expect that they believe X is P, even though this belief does not actually follow from what they said. If someone said "X is P, but I don't believe X is P", we would have a difficult time interpreting the mental state of this person, even though they have expressed a proposition with coherent truth conditions. We can say that *beliefs* are implications of *assertions*. Likewise, if someone says "X is wrong", we would expect that others have reason to refrain from doing X, even though this normative judgment does not actually follow from what they said. If someone said "X is wrong, but I don't judge that anyone ought to refrain from X", we would have a difficult time interpreting his mental state. Thus, we can say *reasons* judgments are implications of *moral* judgments.

Features (2) and (3) are necessary for any normative domain in any culture in order for it to qualify as a moral system. A hypothetical normative system in a hypothetical culture that did not make claims about the appropriateness of moral emotions and social sanctions would not count as a moral system. It also seems that (1) must also be necessary in any conceivable society's morality. It does not seem sensible for someone to genuinely endorse shaming a certain behavior and endorse feeling guilt for performing that behavior, while simultaneously not prescribing agents to not perform the action. In fact, that seems to be what it is to shame a certain behavior, i.e. that you prescribe others to not do it. Therefore, if you endorse shaming a certain behavior, then you endorse prescribing others not to do it.

The first feature (1) makes a claim about the reasons that individual agents have. It is the "agents ought not X" or "agents have reason to not do X" related to moral judgments that X is immoral. This can be be most easily seen in cultures where *discussion* and *argument* play a key role in moral reasoning and communication. Moral argument involves an intention to motivate changes in the attitudes of other agents. We prescribe others to adopt other attitudes, whether these be intentions, dispositions of moral approval/disapproval, anger, guilt, etc. However, what is particular about moral *argument* is that we don't seem to be interested in *merely* motivating such changes. Mere motivation can be achieved with emotional manipulation, rhetorical tricks, threats, force, etc. We seem to be trying to *rationally* motivate a change in attitudes, i.e. we want to expose to them a reason that they have which they were not receptive to. In fact, it is difficult to imagine a society where moral judgments don't play this same role; surely, someone who believed that X is wrong would believe that other agents had *reasons* to not perform the action. It would seem incoherent for this to not be the case. One moral system that focuses on this aspect of the normativity of moral judgments is ethical egoism.

The third feature (3) makes a claim about the reasons from the societal perspective rather than the individual. Is is the "society in general has reason to blame, disapprove or otherwise prohibit for X-ing" realted to moral judgments that X is immoral. This tells us to focus on the reasons of society at large, perhaps by appealing to the personal reasons of society collectively. "Collectively" here must refer to some way of encapsulating the interests of people in general without merely reducing to the interests of *everyone* (e.g. perhaps by looking at the interests of most people, by aggregating everyone's interests, etc.). There are two explanations for why we can't do that: firstly, if we sought agent-neutral principles (e.g. "all agents ought to X", where X makes no reference to some feature of the acting agent), it doesn't seem that there are any behaviors that would satisfy the personal reasons of *everyone*; and, secondly, if we sought agent-relative principles (e.g. "all agents ought to X", where X does no specific reference to some feature of the acting agent), then it would just reduce to (1). So these principles must be receptive to some encapsulated summary of the interests of all agents in society. This is the *impartial* feature of morality. A moral system that focuses on the collective aspect of the normativity of moral judgments is utilitarianism (where the encapsulation function is an aggregation over the well-being of all involved creatures).

The problem is that both of these features of moral judgments have detestable consequences if considered on their own. The first feature (1) only appeals to *personal* reasons, i.e. goal-oriented, desire-based, prudential, etc. reasons. The issue is these reasons don't seem to give the interests of others any intrinsic weight. One's goal-oriented or prudential reasons can be in principle wholly independent of the interests of others. One would be morally obligated to take into account the interest of others only if they (a) happened to care about the interests of others intrinsically, or (b) doing so would promote their other intrinsic interests. This does seem to be the normative force of moral judgments. On the other hand, focusing what is reasonable from the perspective of some encapsulation of society seems does not necessarily secure the seperateness of persons. Because it does not consider the interests of *everyone*, it seems to allow for some principles to be moral even though they might grossly violate the rights of particular agents if doing so provides some greater increase in the satsifaction of collective goals in society.

The solution is to somehow strike a balance between the two features. The problem is that these two considerations pull in opposing directions. One way of doing this is to focus on individual personal reasons combined with some additional constraint that indirectly includes the interests of other agents. For example, you could consider the individual personal reasons a person has for selecting certain principles to govern society, while ignorant of their particular features in the society (Rawls). They would be placed behind a veil of ignorance where they don't know what their race, gender, wealth, talent, etc. will be in society. In this circumstance, an individual would have personal reason to consider the interests of everyone because (for all they know) they might be in any of those positions. Another might be to consider the individual personal reasons a person has for selecting a principle, assuming everyone followed that principle (Contractarianism, Gauthier, etc.). This can handle cases where everyone has individual reason to X over Y, but if everyone collectively did X over Y, then everyone would be severely worse-off than if everyone did Y (prisoner's dilemma). Kant is concerned with what you could rationally will *to be a universal law* which all other agents would follow. Other similar strategies include Habermas, Hare, etc.

??? question "Other interpretations of impartiality"
	Impartiality: Moral reasons can be distinguished from practical reasons in that the moral reasons are reasons for action with some impartiality constraint (e.g. science = interpsonal reasons for belief). Possible conceptions of moral norms
	- Individual practical reasons that everyone individual has.
		- Korsgaard: Valuing anything requires valuing the capacity to value.
		- Denial of Metaphysical Egoism: There is no valid distinction to make between different individuals.
	- Aggregation
		- Reasons that follow after considering everyone's individual goodness. 
		- Maximizing the total utility of everyone (e.g. utilitarianism).
		- Technical problem with aggregate
			- The idea of there being a numerical assessment of well-being is dubious. What would the numbers mean?
		- Intuitive problems with aggregation
			- If we must choose between causing one person extreme harm and N people minor harm, then it the size of N doesn't matter.
			- If we must choose between causing one person harm and N people comparable harm, then it the size of N does matter.
			- If we must choose between causing one person extreme harm and N people serious but not as extreme harm (e.g. paralysis versus loss of limb), then does the size of N matter? There may be no determinate answer to cases like this.
	- Veil of Ignorance
		- Personal reasons a person has under the veil of ignorance.
	- Contractualist:
		- Norms no one could reasonably reject as a basis for unenforced, informed cooperation given that one has such a desire.
		- This does suggest that all agents should be treated as ends in themselves? (maybe not as strong as in the Kantian sense). If agents are not ends, then why does it matter if they can reasonably reject a principle?
	- Universalization:
		- Maxims that A can rationally willed to be a maxim followed by agents. This requires:
			- It be conceivable for the maxim to be universalizable, and
			- It be possible for someone to rationally will the maxim to be universalizable. E.g. Kant says (1) for any agent A, A's ends sometimes require help, (2) if all agents adopted a maxim whereby they never helped anyone, then this would frustrate A's end, therefore (3) A cannot rationally will a maxim that prescribe agents to never help.
		- Necessary but not sufficient.
			- Provides some formal constraints on possible maxims.
			- May need to be supplanted (e.g. with Contractualism) to account for immoral ends which are conceivable and rational.
		- Cannot account for moral actions that are inconceivable as universal laws
			- e.g. Donate to charity more than the average
		- Cannot account for immoral actions that are conceivable/rational as universal laws
			- e.g. Ritualized bullying for newcomers.
		- Too stringent, e.g. Never Lie
		- Non-rational creatures have no moral consideration
	- Kantian Contractualism (from Parfit)
		- Everyone ought to follow the principles whose universal acceptance everyone could rationally will.
		- "An act is wrong unless such acts are permitted by some principle whose universal acceptance everyone could rationally will"
		- Different from standard Kantianism which says "follow principles whose universal acceptance YOU could rationally will."

The problem is that these strategies focus on reasons as such or rationality as such, and then try to indirectly draw a path toward moral reasons by incorporating non-moral considerations. It is true that morality is concerned with these impartial considerations. However, moral reasons cannot be reduced to non-moral reasons in some strange situations where agents have incentive to weigh the interests of others. Instead, moral reasons have to be identified by a particular *aim* that agents have, namely a *moral* aim. Some agents do not have the aim of being moral. Such positions also eliminate the separateness of persons.

The other theories may be useful in that their conclusions may overlap with actual moral reasons, and considering their scenarios may fuel moral motivation. But (1) this overlap is only approximate. These theories can do a very good job at excluding morally irrelevant considerations from our reasoning (i.e. those that focus on an agent's particular situation). However, the considerations that remain (i.e. our personal self-interest) may not be genuinely moral reasons. The strategy of excluding morally irrelevant features needs to go further into we focus precisely on the uniquely moral source of our reasoning. And (2) the motivation from considering these scenarios only makes sense *because* of a prior *direct* interest in moral reasons. If we did not have this *direct* interest, then the above scenarios would have no motivational force. To illuminate the nature of morality, we need to *directly* find the interest that characterizes moral agents.

The aim is a *contractualist* characterization of moral reasons that focuses on *intersubjective justifiability*. We are aiming to find principles for regulating behavior that can be justified to others. But not principles that can be justified to *everyone*. In particular, we are seeking principles that can be justified to everyone *who also has the aim* of seeking such principles (i.e. everyone with the aim of seeking principles that could be justified to others). This satisfies the force of both (1) and (3). (1) is satisfied because we appeal to the personal reasons of certain agents, namely *moral* agents, i.e. agents with the aim of finding principles with intersubjective justifiability. In fact, this interest is an implicit assumption in moral arguments; it wouldn't make sense for someone to engage in a moral discussion if they had no interest in justifying themselves to others. It also satisfies the force of (3) because does not *just* depend on an individual's personal reasons. It addresses the interests of society at large, particular those members of society with an interest in intersubjective justifability. This also avoids the downsides of focusing solely on either of the two features. It avoids the downside of (1) because it doesn't focus on reasons *as such* but rather the reasons that relate to a particular *moral* aim. It also avoids the downsides of (3) because it is concerned with what is reasonable from the perspective of *each individual* with that aim.

This characterization is only true for the moral system of societies where *argument* and *discussion* play a key role in moral reasoning, i.e. any society where *reasons* are posited to explain why agents ought to follow moral duties. As stated earlier, this seems like it would cover any society that has anything resembling a moral system. However, conceivably there could be a moral system that didn't involve judgments about what individuals ought to do (lacking feature (1) above), and just had a standard for shaming the behavior of people. It is difficult to imagine how this would work with rational creatures who have the capacity to ask *why* they are commanded to perform certain actions (surely, *reasons* would have to be provided?).  

## Content

The task is now to determine the substance or content of this characterization of moral reasons. We need to address some variation of the following questions:

1. Which intentions are morally wrong?
2. Which considerations are relavent when deliberating what is morally wrong?
3. How ought one deliberate about what is morally wrong?

This task will be performed later. Before that, however, we can still give a general theory of truth for all moral systems, even those which do not adhere to the contractualist characterization given above. This can be done by treating moral judgments like aesthetic judgments. 

### Sentimentalist

This concerns the attitudes or emotions (e.g. disgust, blame, shame, resentment, guilt, anger, etc.) involved in moral judgments and makes them akin to aesthetic attitudes. These would be any moral judgments we hold regarding a certain behavior despite not judging that they have any intersubjective justifiability. For example, we might mantain a disgust reaction to, e.g., a man has sex with a corpse, even if it isn't unjustifiable given the aim of intersubjective justifiability (e.g. if we somehow know that this doesn't affect the man's relationship with others, that allowing this in society will not have negative effects, etc.). This is not to say that these judgments do not regularly figure in moral arguments. Sometimes they do. But when they do figure in, the argument merely concerns systemezing whatever morally optional sentiments we have ("optional" in the sense of not being necessary from the perspective of intersubjective justifiability). 

There are possibly two ways of establishing truth for these kinds of moral judgments: (1) dispositionalism and (2) rational sentimentalism. Dispositionalism focuses on the refined attitudes that an agent would have under certain idealized conditions, e.g. full information, full deliberation/reflection, full experienced, under a sound state of mind, etc. Rational sentimentalism focuses on what is *constituitive* of moral emotions to determine the appropriate object of said emotions. 

Dispositionalism states the following: one has reason to feel attitude R with regard to X if and only if X is such as to elicit R in circumstances C. The circumstances C can be given several formulations such as, e.g., normal circumstances. This is similar to the truth conditions for when an entity is a certain color; i.e. X is red if and only if X is such as to elicit the perception of redness in normal humans under normal circumstances. However, moral judgments must be based on the attitudes in *idealized* circumstance rather than "normal" circumstances because of the following problems with using "normal" circumstances: (1) It cannot be used to criticize conventional or one's present - assuming they are normal 0 attitudes, (2) It doesn't explain why one's evaluative judgments influence their attitudes, whereas one's judgments about color concepts don't influence their color perceptions, and (3) It doesn't explain why moral truth can be discovered a priori under reflection, whereas truth about color concepts cannot.

Thus, moral truth depends on certain agent's counterfactual attitudes under idealized conditions. Truth could either be specific to an individual's idealized attitudes or the idealized attitudes of normal agents. If the latter, this may allow for some rigidifcation which creates a universal standard for truth common to all agents in the actual world. Regardless, because correctness depends on contingent psychological sensibilities, correctness is not mind-dependent in a way that allows for robust objectivity. This is similar to aesthetic judgments in general (think ettiquette) and perhaps secondary properties. Truth would be based on a refinement of one's attitudes with experience, imagination, deliberation, etc. It doesn't seem that there would standards for moral correctness independent of moral assumptions. Thus, reflective equilibrium seems to be the dominant method for seeking moral truth.

Rational sentimentalism states: one has reason to feel R with regard to X if and only if X has the properties ascribed by R. For example, to fear X involves some sort of perception that it is dangerous. This is constitutive of fear, and there is clear evolutionary reason to develop such a psychological faculty. One has reason, then, to fear X if and only if X is dangerous. Moral emotions, e.g. blame, resentment, guilty, etc., are emotions like that. Unforunately, there may in fact be no constituitive standards of attitudes such as anger, resentment, etc. apart from moral judgments that something is morally wrong. That is, it may not be possible to describe the constituitive features of moral attitudes using non-moral terms.

### Contractualism

1. Which intentions are morally wrong?
2. Which considerations are relavent when deliberating what is morally wrong?
3. How ought one deliberate about what is morally wrong?

This deals with the level of analysis between the meaning of normativity and substantive moral theories. It deals with the nature of motivational force of *moral* reasons, i.e. their relation to normativity generally. This was spelled account was spelled our earlier. Now, to discuss what this implies about the standards of correctness, relevant considerations, and forms of justification of moral systems. 

Justification: the justification of a moral system cannot reduce to "I just prefer this". E.g. even if the system is impartial like utilitarianism. If the justification is "I prefer this system", this provides no justification for the moral system. Utilitiarianism must be shown to be reasonable to all parties involved. It is a substantive question whether this can be done. Otherwise, Utilitarianism would be equally as justified as "everyone maximize my interests". Axioms cannot be justification.

??? questions "Questions"
	1. What is the object of inquiry? Idealized standards for behavior or idealized standards of societal (dis)approval?
	2. How to distinguish between the content of morality and ideal law? The former concerned with societal disapproval and the latter concerned with coercion.
	3. How to ground an account of wrongness and appropriateness of societal disapproval that does not just reduce to the benefits of societal disapproval? What's the relation between what makes something appropriate to disapprove (as an assessment of historical behavior) and having reason to blame it (as a tool for promoting benefits)? Which is prior?

Back to the contractualist characterization of moral reasons based on intersubjective justifiability: moral principles are principles that could be justified to everyone with the aim of finding such principles. In other words, these are principles that could be justified to everyone in group G, where group G is defined as the group interested in principles that can be justified to everyone in G. There are possibly objective standards of correctness for this form of moral judgments, as there is a fixed goal to which there are objectively (in)correct means for satisfying it. This form of moral correctness is the form that grounds claims of *justice*, i.e. when political institutions are *wrong* in a way stronger than just saying they warrant blame/shame. Thus, the content of intersubjective justifiability extends beyond morality in the narrow sense of blame and shame (e.g. it also sets standards of correctness for ettiquette and justice).

Note that these standards are not motivating for (even ideal versions of) all individuals/societies, particularly those individuals/societies that don't use discussion/argument/reasons (rather than rhetoric, emotions, threats, force, divinity, etc.) as a means to persuading others to adopt certain moral norms. Such norms would still be applicable to these parties (i.e. we could still call them morally wrong), but they could never appreciate these standards, nor would they have any personal reason to do so. This makes sense. We find that we really don't think people are *irrational* per se when they are immoral, i.e. we probably wouldn't acuse Hitler, psychopaths, war enemies, etc. of being irrational when they do something we find immoral. And we don't even acuse them of being irrational for believing that they're morally justified (assuming their judgments are consistent with the judgments of their idealized selves). We acuse them of being irrational when they try to justify their actions to us in moral *argument* and *discussion*. 

Given this characterization of moral reasons

1. Moral discussion concerns what *norms* to accept in society. Particular actions are assessed secondarily based on their conformity with those norms. This allows for some rule-consequential reasoning about the justification of norms, which is not possible when focusing strictly on actions. This is not to say that all justifications must be rule-consequentialist. There might also be deontological justification of certain rules.

2. The content/Application of the norms must be impartial. (1) The application of the norms cannot depend on the particular agent, but rather on particular circumstances. E.g. there cannot be a norm that says, "I am morally required to X, but you are morally required to Y", unless it can be shown that there is a relavent difference betwen the two persons. (2) The content of the norms cannot make reference to any particular persons, but rather particular circumstances. E.g. everyone has a moral duty to maximize the pleasure of person A.

3. Norms are justified not because they are reasonable to an agent with a particular point of view. They are justified because they can be reasonable from the "moral point of view". Because all values are not teleological and because there can be reasons that dismiss the relevance of other reasons, this means that certain considerations can be excluded from counting as moral reasons. Here are some constraints on the relevance of certain principles for moral reasoning:

- Personal Reasons: only an agent's personal reasons can be reasons to reject a principle. Personal reasons include the well-being of the individual, and other considerations as well - e.g. their desires, goals, etc. They do not include impersonal reasons (e.g. someone who has a concern for the environment) or interpersonal reasons (e.g. someone who has a concern with the interests of others).
- Fairness: considerations that are indexical can be dismissed. E.g. that A in particular is harmed is not a reason against a principle if all alternative principles would result in a comparable or worse harm for others. I.e. this can justify rejecting a principle that burdens one even if that burden must be shared by someone under all alternative principles. E.g. imagine some people have to share a burden and we have two possible distributions: (1) the burden is distributed randomly or (2) the burden is allocated based on race. (2) can be rejected not because of the burden it distributes but because of the lack of respect it places on persons.
- Degenerate Interests: Interests that either (a) neglect the interests of others or (2) are based upon a desire for the harm of others can be dismissed.
- Responsibility: Harm to those who are responsible is more justifiable tham harm to those who are irresponsible. E.g. if a principle would impose a burden on people by virtue of negligence, poor intentions, etc.
- Aggregation: aggregation in itself doesn't matter. See T.M. Scanlon
- Hard question: how to handle norms that harm some and help others.

Issue regarding the relevance of the consequences of accepting a principle. In a certain sense, the consequences of accepting a certain principle is relevant, e.g. the harm caused by accepting a principle that permitted the raping unconscious persons is a reason against accepting the principle. On the other hand, it seems that sometimes the consequences don't matter, e.g. imagine that an evil demon would destroy the world if a certain principle were accepted. That may be a reason against the principle, but the wrong kind of reason against the principle. 

Note that principles must be reasonable not to agents in some veil of ignorance, but rather to agents in the actual world. Behind a veil of ignorance, it might be reasonable for all to accept a princple that resulted in a state where well-being was 80 and 60 instead of 100 and 40 (imagine there are two people, A and B). However, in the actual world, if A=100 and B=40, it would be reasonable for A to reject a principle where force was used that resulted in A=60 and B=80, even though this end state is superior.

A plausible system of analysis is as follows:

1. In an ideal world, determine the right-making features of a norm regulating *behavior*. I.e. imagine we are thinking about how we are to behave amongst one another in a perfect world where everyone will honor the rules. This will be determined by thinking of whether the norm has intersubjective justifiability. This may partially depend on the consequences of the norm, but also about deontological features. This is not consequentialist in the following way:
	(1) For a particular agent, their reasons need not be consequentialist. For example, this is true insofar as the agent accepts that they have reason to hold certain attitudes, attitudes that cannot be reduced to merely promoting some desired state of affairs. For example, an agent might value friendship, meaning they think they have reason to be loyal to their friends, care about their interests, etc. These reasons are not reasons to promote some state of affairs. This would imply that there could be some end state of affairs that would justify them betraying these current reasons to reach that desired state, but this need not be what an agent has reason to do.
	(2) For groups of individuals, again, there is no end state of affairs that is most desirable that we are trying to reach. The procedures matter for their own sake. Principles can be wrong if they permit wrong actions, even if that principle results in a better state of affairs. We do not lose the separateness of persons here.
	
2. In the actual world, determine whether we accept a norm for the regulation of people toward follow the ideal behavior above. This will concern when it is appropriate to apply sanctions (e.g. stigmitization, blame, force, etc.) to people. Whether a norm should be allowed on this level of analysis will depend on only on the consequences of the norm. However, it cannot be justified on any consequences. The only consequences that matter is the efficacy of the norm in getting people to follow the ideal behavior above. Norms are simply the mechanisms for motivating people away from immoral behavior.
	- E.g. a norm for regulating blame would not be justified if it, say, increased pleasure but had no affect reducing immoral behavior defined above. Because of this we can say that we shouldn't accept a norm in the actual world that blames people for X (if, say, an evil demon would destroy the planet if we accepted the norm), while still saying that X is wrong (because it would be forbidden in an ideal contractual situation mentioned above).
	- E.g. multiple parties cooperate in an immoral system. But norms that blamed this system will have more of an impact depending on who is blamed. Even though all parties actions are wrong (they are equally blameworthy), we should spend time blaming one rather than the other. E.g. there is a high rate of gang violence among individuals raised in an honor culture. The violence is wrong, and societal encouragement of violence is wrong. But we should establish norms that prevent encouraging kids to join gangs. Note two points (1) establishing norms that prevent the encouragement of gangs/violence might also involve blaming gang members, and (2) even if that's not true, there might be independent reason to blaming gang members if it reliably reduced gang violence (regardless of whether it reduced societal acceptance of gang violence).
	=> Should we accept a norm that permits blaming a person who did nothing wrong if it leads to less people doing wrong actions? Not really sure.
	=> This is actually incorrect. Whether blame is appropriate or not needs to be something that is considered in the first stage. Otherwise, how do we distinguish between the legitimate content of morality versus the legitimate content of law. In both cases, they are justified on the grounds of intersubjective justifiability. The difference between the content of the two systems is in the particular attitudes (e.g. blame, resentment, coercion, force, etc.). And these need to be considered in the first stage. The fear with including blame in the first stage was that it might be reasonable to accept a norm admistering blame to innocents, or witholding blame from the guilty depending on the consequences, and so there would be a divergence between what is reasonable to blame and what is actually *blameworthy*, and we would not have an independent characterization of blameworthiness.
	=> Maybe not, add the following points:
		1. To distinguish between different standards in (1), we need not consider blame, shame, disapproval, etc. We need only consider what kinds of relations the norms are to apply to. The purpose of the norms will be to guide the interactions of various kinds of relationships. The relationship may be one of friendship, ettiquette, general interactions, coercive interactions, etc. The content of law versus morality (or friendship, ettiquette, etc.) will be determined based on what's justified from a given relationship.
		2. Regarding (2) above, in addition to whether the norm has an influence on the behavior of people, it also depends on whether the target of the blame is in our moral community. I.e. it makes no sense to blame or disapprove of people not in our moral community, regardless of the possible positive consequences of the blame. E.g. enemies in war, competing tribes, alien attacks, etc. make it inappropriate to attribute blame to others because the opposing parties might be in different moral communities and thus not interested in jutsifying themselves to each other. Blame here would be inappropriate even if establishing such a norm might have positive consequences on regulating the behavior within each group's individual community. We can still infer that their behavior would be wrong though. Just as you wouldn't blame a non-friend for being unfriendly to you, you wouldn't blame an irredeemably moral non-participant person for being immoral to you. This is not to say that we wouldn't be justified in being unfriendly, untrustworthy or combative against those people. Blame is appropriate only when someone fails to live up to the standards of a relationship that they take themselves to be in.
		3. Related to 2, the activity of blaming someone is not only to judge them blameworthy (the judgment that someone is blameworthy differs conceptually from the activity of blaming), i.e. that they are wrong, but also that the person has impaired our relationship with that person. Depending on the relationship, there will be different standards and thus violations that impair a relationship, e.g. friendship versus moral community versus romantic relationships, etc. 
	=> In total, blame involves the following components
		a. Expressive of a con attitude, e.g. anger, resentment, disgust, etc.
		b. Prescribing behavior of other people
		c. Judgment that someone has acted wrongly
		d. Judgment that someone has impaired a relationship
		- Focusing on (b) leads to connecting the appropriateness of blame with its utility, either in promoting better states of affairs generally, or in encouraging people (either the individual being blamed or society in general) to avoid doing wrong actions. Focusing on (c) and (d) leads to connecting the appropriateness of blame with the merit of one's actions, independent of the consequences of blaming them.
	=> This must be re-written

3. In the actual world, determine whether we apply the norm to a particular individual in a particular case. This will simply be based on whether the circumstances make it appropriate to apply the norm, i.e. on whether the content of the norm specifies that the current circumstances are the target. It does not matter if the particular application is ineffectual in its consequences (e.g. if the norm forbids raping corpses, in any particular circumstances, whether we blame someone for it might not make a difference). So really this level of analysis doesn't really do much work and we can just look at (1) and (2).
	
These distinctions are similar to consequentialist systems which distinguish between ideal behaviors and decision theories. E.g. Even act-consequentialist say that it is a theory about what conditions makes actions *right* or *wrong*. This is distinct from whether those conditions should be taken into account for any particular agent when deciding what to do (e.g. if it is infeasible for an individual to weigh the consequences in any particular circumstances. 
- So there are three levels of analysis for act-consequentialists: (1) the best state of affairs, (2) right/wrong actions, and (3) decisions about what we ought to do. 
- Rules consequentialism for norms for *behavior* has the following levels of analysis: (1) the best state of affairs, (2) good/bad norms for behavior, (3) right/wrong actions - based on their adherence to the norms, (4) decisions about what we ought to do - probably just refers back to the norms.  
- Rule consequentialism for norms of *moral emotions* has the following levels: (1) the best state of affairs, (2) good/bad norms for moral emotions, (3) right/wrong actions - whether it's the appropriate object of blame given the norms, (4) decisions about whether we ought to blame - probably just refers back to the norm.
- The contractualist analysis: (1) right/wrong actions - just depends on intersubjective justifiability, (2) good/bad norms for moral emotions - depends on whether it promotes/diminishes undesirable behavior.

## Extensions of other systems

### Constructed normative domains

Imagine social games with their own personal rules.
We can use those rules as standards of criticism without rationally criticizing an agent.

Or consider the constructed standards of relationships
- To stand in a certain relation to someone (e.g. friend, lover, partner, cooperator, self-respecting individuals, etc.) has certain expectations for behavior by the parties involved. To the extent that someone violates these expectations, it is appropriate to not extend those behaviors to them, since they would now stand outside of the relationship. 
- This might work for friends and ettiquette (i.e. to the extent that someone doesn't uphold the standards of good friendship, ettiquette, it is appropriate to not treat them as a friend, or with ettiquette). Maybe it also works for aesthetic morality. But maybe this doesn't work for forceful morality (maybe if someone is immoral, we still have moral obligations towards them, i.e. we can exclude them from society, but we cannot kill them).

Other constructed normative domains
- Prudential Rationality: there are no independent standards for bodily movements. But insofar as one is deliberative and reflective and evaluates their own actions, i.e. one is a rational agent, there are standards for behavior.
- Games: there are no independent standards for moving small pieces across a checkered board. But insofar as one is playing chess, there are constituitive standards of the activity that entail standards of correctness.
- Relationships: there are no independent standards of kindness, respect, loyalty, etc. with regard to how to treat others. But insofar as one is being a friend, partner or other normatively-laden relationships, there are constituitive standards of the activity that entail standards of behavior.
- Communication: there are no independent standards of speaking or language. But insofar as one is communicating as a means to transfer ideas to someone else, there are standards of correctness.
- Conversation: To engage in a *discussion* or *conversation* with someone, one implies that they will listen to the other party, not cut them off, etc. even though there may be no independent reason to do thees things. 
- Negotiation: To engage in a *negotiation* implies that people are willing to sacrifice or deviate from their most preferred plan to help satisfy the interests of others. Now, one might have no reason to do these things (i.e. if they had full power), but then they wouldn't be negotiating. They would be bad negotiators. 
- Morality: there are no independent standards of behavior. But insofar as one is engaged in moral argument as a procedure for finding *impartial reasons*, or reasons that all can accept, there are constituitive standards of correctness.

Communicative constraints regarding reasons for belief and/or conversations/arguments generally.
- Burden of proof depends on the speaker who asserts a claim, not on the mere content of the proposition. That someone asserts a claim has a burden of proof cannot be entailed from the content of the propositions expressed. When Mike says "God Exists", it follows that he has reason to follow the rule, even though this reason is not trivially included in the rational extension of his motivational set for beliefs.
- Even though an anecdotal experience might provide one with a personal reason for belief, it would not provide everyone with a reason for belief (e.g. a scientific reason).
- How to know what the standards are? We check what would happen if everyone accepted it. If no one accepted a burden of proof, the central goal of argument would be defeated.
- Don't interrupt others, cut them off, etc. Listen to them.

### Intersubjective Justification

Many normative domains are concerned with what can be justified to different agents, not any individual agent. E.g.

- Personal experience / intuition / presuppositions might provide an individual agent with reasons for believe. But they cannot independently provide anyone else with a reason for belief (unless the other agent believes the other person's experience/intuition/etc. are accurate). Science is a system that searches for intersubjective justification for beliefs. Thus, such instances are not scientifically justified.
- That a principle benefits A at the expense of everyone else might be a reason for A to adopt the principle, but that is no reason for anyone else to accept it (e.g. a principle that said everyone has a moral obligation to maximize my pleasure). Thus, such a principle cannot be morally justified.

### Golden Rule

- Golden Rule: Treat others in the way you would want to be treated.
	- Upshot: don't kill if you wouldn't want to be killed. 
	- Problem 1: what if I would have different desires than the recipients? E.g. a masochist wouldn't mind being attacked.
	- Problem 2: what if I would have desires that people treat me unreasonably? E.g. I would want others to always benefit me at the expense of others. 
- Moral Emotions: Treat others in the way that you would not blame someone for treating you
	Upshot: You can treat others in a way that you would dislike, so long as you wouldn't blame them for it.
- Rationalized: Treat others not based on counterfactual desires, but on our counterfactual reasons for blame.
	- Upshot: don't kill if they don't have reason to want to be killed (e.g. even if they're in an intoxicated state where they want to be killed).
	- Doesn't solve either problem. Just idealizes desires/blame.
- Inversed: Treat others in the way they want you to treat them or would not blame you for.
	- Upshot: don't attack if the person doesn't want to be attacked.
	- Solves Problem 1.
- Abstracted: Treat others in accord with principles any rational agent would endorse, independent of their actual circumstances, desires, interests, etc. (i.e. veil of ignorance), if the agent knew he had a chance of any combination of circumstances or desires.
	- Upshot: don't kill it would be forbidden by any principles that no one could rationally reject.
	- Solves Problem 1 and Problem 2.
	- Reasons from Ignorant self-interest =/= reasons from morality.
- Moralized: Treat others in accord with principles no rational agent could rationally reject, contingent upon that agent having a goal of finding such principles that others couldn't rationally reject.

### Negotiation

- Normal negotions: 
	- Two parties are interested in trading goods. 
	- Each party has a ranked set of alternative systems of trade they would be willing to accept.
	- e.g. one party is willing to pay no more than $100 for some good X. The other is willing to sell X for no less than $50.
	- A good negotiation is one where X is sold for somewhere between $50 and $100.
	- The exact number to make the sell is indeterminate.
	- Agents may not have *reasons* to be good negotiators, i.e. if one party could coerce the other, that might be in their best interests.
- Morality features the following augmentations
	- Concerns alternative systems of principles to regulate society; not systems of trade.
	- Impartiality: Abstracts from any particular irrelevant circumstances, i.e. wealth, power, fame, race, etc. 

## Motivation

Both forms of morality explains why agents have reason to endorse certain moral principles. But neither explains why an agent has reason to abide by the duties prescribed by the moral principles. An agent would have such a reason insofar as they either (1) have natural sympathy with the welfare of others (meaning the reasons that others hold would be reasons that they also hold), or (2) judge that their behavior should be consistent with the norms they endorse.

## Objectivity

Morality is rationally optional, but this doesn't diminish its authority:

- Contingent on our interests:
	- Nearly universal: most people interested in impartial/social reasons, having a system of norms regulating blame.
	- Emphasizes the negotiation aspect: people cant just have independent that they claim are objective with no consideration of other person's reasons.
	- Other analogues: Scientific reasons are impartial/social reasons for belief (and thus evidentially optional), but that doesn't diminish its authority.
- Not externally necessarily justifiable, like all normative domains
	- Deduction/induction/abduction/evidence/science cannot be reasonable to someone who doesn't care for those forms of reasoning

There are no judgment-independent standards of moral truth, but that's a good thing:

- People cannot just come with arbitrary principles and claim them as the moral virtues.
- They are forced to take into account the interests of others.
- They are forced to make their demands reasonable to others. 
- Moral reasoning is a process of negotiation rather than of discovery.