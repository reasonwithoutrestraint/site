# Moral Reasons

Where this lies in study of ethics

1. MetaEthics - semantics, psychology, epistemology, metaphysics of normativity and reasons generally.
2. Moral Reasons - The above applied to morality reasons specifically. E.g. the relationship between morality and normativity/rationality, between morality and motivation, etc.
3. Normative Ethics - Systems of broad substantive principles to settle a wide range of moral questions.
4. Applied Ethics - Specific principles to settle specific moral questions.

??? example "Readings"
	Derek Parfit
		- _Reasons and Persons_ (1984)
		- _On What Matters_ (2011)
		- "Justifiability to Each Person" (2003)
		- look up his discussion on "Kantian Contractualism"
	Thomas Scanlon
		- _What We Owe to Each Other_
		- "Contractualism and What We Owe to Each Other"
		- "Contractualism and Utilitarianism"
		- _Moral Dimensions: Permissibility, Meaning, Blame_
	T. Pogge, “What We Can Reasonably Reject” (2001)
	R. Kumar, "Reasonable reasons in contractualist moral argument" (2003)
	- Gilbert Harman, "Moral Relativism Defended"
	- Philippa Foot, "Morality as a System of Hypothetical Imperatives"
	- Essay: Nick Zangwill, "Externalist Moral Motivation"
	- Essay: David Brink, "Externalist Moral Realism"
	- David Gauthier, _Morals By Agreement_ (as in DGR)
	- Habermas,
	- Christine Kosgaard, "Kant's Formulation of Universal Law"

??? question "Some questions"
	- Contractualism rejects that the only relevant consideration for moral reasoning is well-being. It includes anything that people have reason to care about. Some questions:
		1. Can this "cares" be reduced to a common good? It doesn't seem like it. It obviously wouldn't be well-being. Maybe something like goals.
		2. If yes, can we ignore referring to specific kinds of goals? Instead use a language that just refers to intrinsic goals generally without losing importance? Doesn't seem like it. E.g. imagine an agent had two goals: to be entertained and to provide food for his children. It you characterize this just as goal X and goal Y, you seem to lose just how much more important Y is compared to X. 
		3. If yes, can goals be compared numerically? Harms of qualitative differences don't seem comparable, e.g. physical versus emotional harm. Also, significant quantitative differences in harm don't seem comparable on a linear scale, e.g. being tortured versus a pinch can't be compared on some numerical scale such that N instances of a pinch matches 1 instance of torture. Some kinds of harms:
			- Sensation
				- Physical pain
				- Emotional pain
			- Liberty limitation
				- In ability, e.g. losing a limb, mobility, etc.
				- In opportunity, e.g. discrimination, oppression, etc.
	- Establishing a method for establishing moral truth.
	- How can we use conceptual analysis to determine the role of moral norms but not all norms
		-> We can determine the role for norms, but not the role for attitudes.
		-> As for rational *norms* (not the *attitudes*), we can discover them (i.e. instrumental rationality as constituitive), but we must give a non-reductive analysis (i.e. constructivism).

Some disputes:

Epistemology:
1. Moral truths can be determined by reason alone
2. More truths can motivate intrinsically
3. Reason alone cannot motivate intrinsically

Normativity
1. All rational agents have reason to be moral
2. Morality does not depend on any agent's particular desires or interests
3. Reasons for action depend on an agent's particular desires or interests

Personal/Interpersonal
1. All rational agents have reason to be moral
2. Society has reason to socially coerce people into being moral
3. What society has reason to coerce people to do =/= what individuals have reason to do

Substantive vs Formal characterizations - what kind of reasons are moral reasons?
1. Formal characterizations 
	- Moral reasons are reasons in general. 
	- Don't seem to get at what morality is.
	- Prudential reasons => Seem like a bribe, also not categorical
	- Categorical reasons => Also seems like a bribe, seems inappropriate (e.g. rules of logic are not moral)
2. Substantive characterizations
	- Particular kinds of reasons
	- Reasons toward a particular end
	- We can still sensibly ask, Why be moral?

Thomas Scanlon distinctions on blame (from a YouTube series, similar to the one where he mentions the substantive vs formal characterizations above)

## Moral Reasons

Moral judgments are deeply tied to distinctly moral emotions and attitudes - blame and guilt. There are two ways that moral judgments are associated with these basic attitudes. Firstly, agents who make moral judgments directly express those attitudes, or a disposition to have those attitudes in certain situations. Secondly, agents who make moral judgments express a normative judgment that said attitudes are appropriate, i.e. they express their *endorsement* of those attitudes in certain situations. First, let us discuss the motivational implications of moral judgments. When an agent makes a moral judgment, (1) the agent is disposed to regulate his behavior as prescribed by the judgment, (2) he is disposed to feel guilt himself for violating the standard, and (3) he is disposed to blame others for violating the perscribed standard. Oftentimes, the agent thinks others are *capable* of being motivated to regulate their behavior in accord with the norm, i.e. they would be receptive to the reasons favoring the norm. E.g. while it might make sense for A to *dislike* B's preference for ice crean, it doesn't make sense for A to *disapprove* of B's preference for ice cream, but it would make sense if B regarded A as his dietary adviser. This is true even when B's actions might harm A. E.g. it doesn't make sense for A to disapprove of B's attack against him if B would be completely unreceptive (e.g. animal, child, evil person, aliens etc.), but it would make sense for someone who was receptive (i.e. normal rational agent).

Moreover, like all normative judgments, moral judgments are also judgments about when certain attitudes are appropriate. That is, they express an agent's endorsement of blame and guilt in certain situations. They state which attitudes agents ought to adopt. And like all ought-statements, these ought claims are to be explained by the *reasons* agents have for acting as such. So morality consists of a system of moral reasons, and agents who act immorally are not receptive to the moral reasons that apply to them. The task is to characterize what is distinctive of moral reasons, explaining how they differ from normative reasons in general and (possibly) how they differ from practical reasons in general. To judge that an action X is morally wrong implies some variation of the following ought claims: (1) agents ought not to intend X, (2) agents ought to feel guilt for intending X, and (3) an agent does who X would be blameworthy if they were responsible for doing X (i.e. there were no extenuating circumstances that excuse their behavior). Someone who invokes any of these ought-claims judges that there are reasons explaining why agents ought to act as claimed.

These features can be exposed by considering the implicit commitments of moral judgments. Consider an implicit commitment of *assertions*: if someone says "X is P", we would expect that they believe X is P, even though this belief does not actually *logically* follow from their asserted proposition (nor is it an *analytic implication*, s.t. if A believes "X is P" we can deduce that they also believe that they believe "X is P"). If someone said "X is P, but I don't believe X is P", we would have a difficult time interpreting the mental state of this person, even though they have expressed a proposition with coherent truth conditions. We can say that *beliefs* are implications of *assertions*. Likewise, if someone says "X is wrong", we would expect that others have reason to refrain from doing X, even though this normative judgment does not actually follow from what they said. If someone said "X is wrong, but I don't judge that anyone ought to refrain from X", we would have a difficult time interpreting his mental state. Thus, we can say *reasons* judgments are implications of *moral* judgments. Similar remarks apply to how moral/normative judgments imply non-cognitive attitudes.

Features (2) and (3) are necessary for any normative domain in any culture in order for it to qualify as a moral system. A hypothetical normative system in a hypothetical culture that did not make claims about the appropriateness of moral emotions would not count as a moral system. If someone made a judgment that only involved (1), this would seem more akin to a pure prudential or rational judgment, as opposed to a moral judgment. It also seems that (1) must also be necessary in any conceivable society's morality. It does not seem sensible for someone to genuinely endorse shaming a certain behavior and endorse feeling guilt for performing that behavior, while simultaneously not prescribing agents to not perform the action. In fact, that seems to be what it is to shame a certain behavior, i.e. that you prescribe others to not do it. Therefore, if you endorse shaming a certain behavior, then you endorse prescribing others not to do it.

### Blame and Relationships

To understand what it is to judge that an action/person is *blameworthy* (i.e. the appropriate object of blame), we need to understand what constitutes the attitude of blame. There are two (not necessarily conflicting) characterizations of the attitude of blame. Firstly, (1) to blame someone is just to make the the evaluative *judgment* that he has done something wrong. This can be seen as a sober assessment of someone's character. It is a grading of their sets of intentions, desires, goals, etc. against some preferred standard. Insofar an agent's behavior falls short of that standard, we judge their actions to be morally wrong and thus blame them. Secondly, (2) to blame someone is to adopt more of a functional, dispositional or motivational role and is more akin to an emotion. Blaming someone involves several other non-cognitive attitudes. To blame someone is to hold some sort of disapproving attitude with regard to that person, i.e. it is to be adopt the attitude of anger, resentment, disgust, shame, hatred, etc. Blaming someone in this sense can be seen as a social tool to serve as a sanction that motivates others to behave in accordance with an individual's preferred standard. It plays the role of modifying the attitudes of others, of both the person being blamed and other individuals in society. 

While these attitudes are commonly invoked when one engages in blaming someone, there is a more important aspect of blaming that is neither an evaluative judgment nor a social saction. To blame someone also involves a modification of one's attitudes about the person. In particular, when A blames B for an action A judges that his relationship with B has been impaired in some way due to B's actions. A judges that B violated certain expectations that were supposed to govern his actions by virtue of being in that relationship. It suggests that some reconciliation is needed by B to mend the relationship, if that is even possible. Because blame involves more than the judgment that someone or an action is wrong, it is possible to judge an action to be wrong without blaming them. This can happen (1) when an action is wrong but is not seen as bad enough to warrant a revision of attitudes. But it also occurs (2) when someone does something that *would* violate a standard of a relationship if such one existed but such a relationship does not exist (thus they never violating any expectations). 

When (2) this happens and the agent can potentially be in relationships with others, we can still judge their actions to be *blameworthy* (without actually blaming them), because we understand that it would be appropriate to someone to blame them if they were in the appropriate relationship. But when it happens to someone that we see as incapable of being in such relationships (i.e. completely cut off from our moral community, e.g. animals, aliens, sociopaths, enemies in war, etc. or people who are incapable of living up to such expectations, e.g. children, mentally disabled, etc.), we neither blame them nor judge them blameworthy. Similar remarks can be made about disapproval in general. If someone does not belong to our moral community, it does not make sense to *disapprove* of their actions. It makes sense to *dislike* their actions, but disapproval implies that they are receptive to our complaints in some way, i.e. that we stand in a certain relationship with them.

This analysis of blame can account for many of the features of the standards for other kinds of interpersonal relationships we form. E.g. when two people are in a friendship, there are certain expectations and restrictions on how they behave that would not exist if it were not for the relationship. It is expected that one's friends will be loyal, trustworthy, helpful, interested in their well-being, etc. When someone does something that violates these standards, their behavior can be said to be wrong *from a friendship perspective*. However, this would not suffice to blame a person for this behavior, e.g. if a stranger did some of these things (e.g. failed to be loyal to you), you would not *blame* him because you had no *expectation* that he would do so (though you might judge him *blameworthy*, i.e. that an person in the appropriate circumstances has reason to revise their attitudes regarding the person). Blame for being wrong in this way is only appropriate when one is expected to be a good friend; you would revise your attitudes with regard to that person because they have impaired the relationship, e.g. you would no longer trust them, confide in them, become less interested in their interests, etc. which is to stop treating them as a friend. Similar remarks can be said about romantic relationships, ettiquette, etc. 

All relationships can be defined as a system of expected attitudes and dispositions which specify standards for any person within that relationship. Moral wrongness is based on the standards of a *moral relationship*. This is a general kind of relationship that consists of expectations that others' behavior will be constrained by mutual recognition and respect, rather than specific expectations of other more "optional" relationships (e.g. loyalty, love, faithfulness, etc.). The nature of this relationship is characterized by common reasons that we all recognize *as reasons*. The nature of these reasons is spelled by the contractualist formula. We only blame someone when we expected them to be in this relationship. This expectation is a "default" expectation that is only withdrawn in particular cases, e.g. children, mentally disabled, sociopaths, enemies in war, etc. Just as you wouldn't blame a non-friend for being unfriendly to you, you wouldn't blame an irredeemably moral non-participant person for being immoral to you. This is not to say that we wouldn't be justified in being unfriendly, untrustworthy or combative against those people. However, this wouldn't be a *revision* (because we were never friendly, trustworthy, etc. to begin with).

This analysis also determines when blame is appropriate. Blame is appropriate when we have reason to judge that a relationship has been impaired or to revise our attitudes with regard to a person in light of violated expectations. Likewise, judgments of blameworthiness are appropriate when we have reason to believe that anyone in the appropriate relationship would have reason to revise their attitudes as such. Note that this kind of reason is distinct from other consequentualistic kinds of reasons for blaming someone, i.e. the positive benefits of blaming someone (e.g. someone who did an action accidentally or because of non-culpable ignorance, while possibly beneficial to blame - e.g. if people were motivated to be more careful, more informed after seeing this blame - would not actually be blameworthy). While these may be reasons to blame someone, these are secondary reasons (i.e. the "Wrong" kind of reason). The primary reason to blame someone lies in our reason to revise our attitudes. Note also that *this* reason (to revise our attitudes) is also evaluative in nature rather than consequentialist. E.g. if one of the expectations of a relationship is trustworthiness, and someone in the relationship reveals that they are no longer trustworthy because they are dishonest, then this provides the primary reason to revise our attitudes with this person (by no longer trusting them), regardless of the benefits of this revision. 

This analysis of blame also makes blame appropriate even in a deterministic world. There are two senses in which this is true: (1) in the secondary sense of "reasons" for blame which are based on consequences: we have reason to establish norms for blame if it produces good consequences, independent of their free will, and (2) in the primary sense of "reasons" for blame: we have reason to blame someone if someone has impaired a relationship that calls for revising our attitudes of that person. This impairment can occur independent of the consequences of blaming them and also independent of that person's free will. Thus, the revision of our attitudes can be appropriate independently of their free will. For example, it is appropriate to be suspiscous of someone who is untrustworthy, defensive to someone who is insulting, etc. independent of their free will. Likewise, it is appropriate to revise our attitudes towards suspiscion or defensiveness toward someone insofar as they reveal themselves to be untrustworthy or abusive, independent of their free will. If this relevation violates an expected standard of a relationship, then this is enough to justify blaming them.

Ordinarily, these standards apply to an individual only if a person agrees to the standards of the relationship, e.g. friendship, romantic, etc. But are there some that are not optional, e.g. being a parent to one's offspring, moral relationships, etc.

Ordinarily, when people fail to meet the standards of a relationship, we do not adopt the attitudes that constitutute the relationship (e.g. bad friends are not treated like friends anymore). Does the same apply to the moral relationship, e.g. murderers/rapists get no moral consideration.

The task, now, is to specify the moral relationship that we take ourselves to stand in relation to for every other person. See below.

### Possibilities

How to get to various moral theories
- Rationality = Teleological => Consequentialism
	+ Morality = Rationality => 
		+ Rationality = Self Interest => Egoistic Consequentialism
		+ Rationality = Impartial => ~Impartial Consequentialism (e.g. Smith)
	+ Morality = Impartiality => Impartial Consequentialism (e.g. Railton); strip away indexical reasons. Seems like because morality is built off of rationality, then if rationality is consequentialist then so must morality.
- Rationality = Deontological =>
	+ Morality = Rationality => Deontology
	+ Morality = Impartiality; strip away indexical reasons. Are the remaining reasons consequentialist? Do they respect the seperateness of persons?
		+ Moral Reasons = Consequentialist Reasons => Consequentialism
		+ Moral Reasons = Some Deontological Reasons =>
			+ Seperateness of Persons => Deontology
			+ Not Seperateness of Persons => Impartial Consequentialism, ???

0. Standard Personal reasons - I satisfy my interests 
	- Not a moral theory. Doesn't specify rules for everyone.
1. Speaker-personal reasons - everyone satisfy my interests
	- Indexical, so not a moral theory.
	- While not inconsistent, really doesn't make sense. It violates a constraint implicit in normative judgments, that others have reason to accept your principle. E.g. "You are morally forbidden from stealing from me when it benefits you, but I am morally permitted to do so when it benefits me". Strictly speaking, it is not a contradiction, but it does seem weird (like saying "X is true but I don't believe X"). 
====> Below here are substantive questions. The above don't qualify as moral theories. They don't fulfill the role that moral discourse plays.
2. Interpersonal reasons - everyone satisfy their interests
	- While not inconsistent, violates the *interpersonal* nature of moral judgments, i.e. that others make interpersonal demands on the actions of others (and that we are sometimes answerable the demands of others).
3. Impartial interpersonal reasons - everyone satisfy everyone's interests
	- Seems like there are sometimes cases where we can favor our own interests over others.
4. Reasons for action versus reasons for blame/shame

Private vs Public reasons
- See here: https://plato.stanford.edu/entries/reasons-agent/#RelDis
- Agent-relative vs agent-neutral
- Internal vs External: 
	- Internalism -> AR
	- Externalism -> AR/AN
- Intersubjective vs Non-intersubjective (reasons A has s.t. we can communicate this to A)
- Essentially-shared/Not-essentially-shared (reasons we all have)

The first feature (1) makes a claim about the reasons that individual agents have. It is the "agents ought not X" or "agents have reason to not do X" related to one's judgment that X is immoral. This can be be most easily seen in cultures where *discussion* and *argument* play a key role in moral reasoning and communication. Moral argument involves an intention to motivate changes in the attitudes of other agents. We prescribe others to adopt other attitudes, whether these be intentions, dispositions of moral approval/disapproval, anger, guilt, etc. However, what is particular about moral *argument* is that we don't seem to be interested in *merely* motivating such changes. Mere motivation can be achieved with emotional manipulation, rhetorical tricks, threats, force, etc. We seem to be trying to *rationally* motivate a change in attitudes, i.e. we want to expose to them a reason that they have which they were not receptive to. In fact, it is difficult to imagine a society where moral judgments don't play this same role; surely, someone who believed that X is wrong would believe that other agents had *reasons* to not perform the action. It would seem incoherent for this to not be the case. One moral system that focuses on this aspect of the normativity of moral judgments is ethical egoism.

The third feature (3) makes a claim about the reasons from an *interpersonal* perspective rather than an individual or personal perspective. It is the "*others* have reason to blame, disapprove or otherwise prohibit for X-ing" related to a judgment that X is immoral. Unlike personal reasons, interpersonal reasons often involve more than one agent. The question is whether there are grounds for one person (i.e. the judge) to disapprove of another person (i.e. the actor). One way of characterizing interpersonal reasons is by appealing to the personal reasons of society collectively. "Collectively" here must refer to some way of encapsulating the interests of people in general without merely reducing to the interests of *everyone* (e.g. perhaps by looking at the interests of most people, by aggregating everyone's interests, etc.). There are two explanations for why we can't do that: firstly, if we sought agent-neutral principles (e.g. "all agents ought to X", where X makes no reference to some feature of the acting agent), it doesn't seem that there are any behaviors that would satisfy the personal reasons of *everyone*; and, secondly, if we sought agent-relative principles (e.g. "all agents ought to X", where X does no specific reference to some feature of the acting agent), then it would just reduce to (1). So these principles must be receptive to some encapsulated summary of the interests of all agents in society. This is the *impartial* or *interpersonal* feature of morality. One example of a moral system that focuses on the collective aspect of the normativity of moral judgments is utilitarianism (where the encapsulation function is an aggregation over the well-being of all involved creatures).

The problem is that both of these features of moral judgments have detestable consequences if considered on their own. The first feature (1) only appeals to *personal* reasons, i.e. goal-oriented, desire-based, prudential, etc. reasons. The issue is these reasons don't seem to give the interests of others any intrinsic weight. One's goal-oriented or prudential reasons can be in principle wholly independent of the interests of others. One would be morally obligated to take into account the interest of others only if they (a) happened to care about the interests of others intrinsically, or (b) doing so would promote their other intrinsic interests. This does seem to miss the force of moral judgments. It also implies that individuals can have incompatible moral obligations, which seems wrong. On the other hand, focusing on what is reasonable from the interpersonal perspective seems to risk subjugating the individual to the demands of the collective. Because it focuses on an encapsulation of everyone's reasons, thus eliminating the seperateness of persons, it seems to allow for some principles to be moral even though they might grossly violate the rights of particular agents as a means to maximize the collective.

The solution is to somehow strike a balance between the two features. The problem is that these two considerations pull in opposing directions. One way of doing this is to focus on individual personal reasons combined with some additional constraint that indirectly includes the interests of other agents. For example, you could consider the individual personal reasons a person has for selecting certain principles to govern society, while ignorant of their particular features in the society (Rawls). They would be placed behind a veil of ignorance where they don't know what their race, gender, wealth, talent, etc. will be in society. In this circumstance, an individual would have personal reason to consider the interests of everyone because (for all they know) they might be in any of those positions. Another might be to consider the individual personal reasons a person has for selecting a principle, assuming everyone followed that principle (Contractarianism, Gauthier, etc.). This can handle cases where everyone has individual reason to X over Y, but if everyone collectively did X over Y, then everyone would be severely worse-off than if everyone did Y (prisoner's dilemma). Kant is concerned with what you could rationally will *to be a universal law* which all other agents would follow. Other similar strategies include Habermas, Hare, etc.

??? question "Other interpretations of impartiality"
	Impartiality: Moral reasons can be distinguished from practical reasons in that the moral reasons are reasons for action with some impartiality constraint (e.g. science = interpsonal reasons for belief). Possible conceptions of moral norms
	- Individual practical reasons that everyone individual has.
		- Korsgaard: Valuing anything requires valuing the capacity to value.
		- Denial of Metaphysical Egoism: There is no valid distinction to make between different individuals.
	- Aggregation
		- Reasons that follow after considering everyone's individual goodness. 
		- Maximizing the total utility of everyone (e.g. utilitarianism).
		- Technical problem with aggregate
			- The idea of there being a numerical assessment of well-being is dubious. What would the numbers mean?
		- Intuitive problems with aggregation
			- If we must choose between causing one person extreme harm and N people minor harm, then it the size of N doesn't matter.
			- If we must choose between causing one person harm and N people comparable harm, then it the size of N does matter.
			- If we must choose between causing one person extreme harm and N people serious but not as extreme harm (e.g. paralysis versus loss of limb), then does the size of N matter? There may be no determinate answer to cases like this.
	- Veil of Ignorance
		- Personal reasons a person has under the veil of ignorance.
	- Contractualist:
		- Norms no one could reasonably reject as a basis for unenforced, informed cooperation given that one has such a desire.
		- This does suggest that all agents should be treated as ends in themselves? (maybe not as strong as in the Kantian sense). If agents are not ends, then why does it matter if they can reasonably reject a principle?
	- Universalization:
		- Maxims that A can rationally willed to be a maxim followed by agents. This requires:
			- It be conceivable for the maxim to be universalizable, and
			- It be possible for someone to rationally will the maxim to be universalizable. E.g. Kant says (1) for any agent A, A's ends sometimes require help, (2) if all agents adopted a maxim whereby they never helped anyone, then this would frustrate A's end, therefore (3) A cannot rationally will a maxim that prescribe agents to never help.
		- Necessary but not sufficient.
			- Provides some formal constraints on possible maxims.
			- May need to be supplanted (e.g. with Contractualism) to account for immoral ends which are conceivable and rational.
		- Cannot account for moral actions that are inconceivable as universal laws
			- e.g. Donate to charity more than the average
		- Cannot account for immoral actions that are conceivable/rational as universal laws
			- e.g. Ritualized bullying for newcomers.
		- Too stringent, e.g. Never Lie
		- Non-rational creatures have no moral consideration
	- Kantian Contractualism (from Parfit)
		- Everyone ought to follow the principles whose universal acceptance everyone could rationally will.
		- "An act is wrong unless such acts are permitted by some principle whose universal acceptance everyone could rationally will"
		- Different from standard Kantianism which says "follow principles whose universal acceptance YOU could rationally will."

### Contractualism

The problem is that these strategies focus on reasons as such or rationality as such, and then try to indirectly draw a path toward moral reasons by incorporating non-moral considerations. It is true that morality is concerned with these impartial considerations. However, moral reasons cannot be reduced to non-moral reasons in some strange situations where agents have incentive to weigh the interests of others. Instead, moral reasons have to be identified by a particular *aim* that agents have, namely a *moral* aim. Some agents do not have the aim of being moral. Such positions also eliminate the separateness of persons.

The other theories may be useful in that their conclusions may overlap with actual moral reasons, and considering their scenarios may fuel moral motivation. But (1) this overlap is only approximate. These theories can do a very good job at excluding morally irrelevant considerations from our reasoning (i.e. those that focus on an agent's particular situation). However, the considerations that remain (i.e. our personal self-interest) may not be genuinely moral reasons. The strategy of excluding morally irrelevant features needs to go further into we focus precisely on the uniquely moral source of our reasoning. And (2) the motivation from considering these scenarios only makes sense *because* of a prior *direct* interest in moral reasons. If we did not have this *direct* interest, then the above scenarios would have no motivational force. To illuminate the nature of morality, we need to *directly* find the interest that characterizes moral agents.

The aim is a *contractualist* characterization of moral reasons that focuses on *intersubjective justifiability*. We are aiming to find principles for regulating behavior that can be justified to others. But not principles that can be justified to *everyone*. In particular, we are seeking principles that can be justified to everyone *who also has the aim* of seeking such principles (i.e. everyone with the aim of seeking principles that could be justified to others). This satisfies the force of both (1) and (3). (1) is satisfied because we appeal to the personal reasons of certain agents, namely *moral* agents, i.e. agents with the aim of finding principles with intersubjective justifiability. In fact, this interest is an implicit assumption in moral arguments; it wouldn't make sense for someone to engage in a moral discussion if they had no interest in justifying themselves to others. It also satisfies the force of (3) because does not *just* depend on an individual's personal reasons. It addresses the interests of society at large, particular those members of society with an interest in intersubjective justifability. This also avoids the downsides of focusing solely on either of the two features. It avoids the downside of (1) because it doesn't focus on reasons *as such* but rather the reasons that relate to a particular *moral* aim. It also avoids the downsides of (3) because it is concerned with what is reasonable from the perspective of *each individual* with that aim.

This characterization is only true for the moral system of societies where *argument* and *discussion* play a key role in moral reasoning, i.e. any society where *reasons* are posited to explain why agents ought to follow moral duties. As stated earlier, this seems like it would cover any society that has anything resembling a moral system. However, conceivably there could be a moral system that didn't involve judgments about what individuals ought to do (lacking feature (1) above), and just had a standard for shaming the behavior of people. It is difficult to imagine how this would work with rational creatures who have the capacity to ask *why* they are commanded to perform certain actions (surely, *reasons* would have to be provided?).  

Note that these standards are not motivating for (even ideal versions of) all individuals/societies, particularly those individuals/societies that don't use discussion/argument/reasons (rather than rhetoric, emotions, threats, force, divinity, etc.) as a means to persuading others to adopt certain moral norms. Such norms would still be applicable to these parties (i.e. we could still call them morally wrong), but they could never appreciate these standards, nor would they have any personal reason to do so. This makes sense. We find that we really don't think people are *irrational* per se when they are immoral, i.e. we probably wouldn't acuse Hitler, psychopaths, war enemies, etc. of being irrational when they do something we find immoral. And we don't even acuse them of being irrational for believing that they're morally justified (assuming their judgments are consistent with the judgments of their idealized selves). We acuse them of being irrational when they try to justify their actions to us in moral *argument* and *discussion*. 

## Content

The task is now to determine the substance or content of this characterization of moral reasons. We need to address some variation of the following questions:

1. Which intentions are morally wrong?
2. Which considerations are relavent when deliberating what is morally wrong?
3. How ought one deliberate about what is morally wrong?

??? questions "Questions"
	1. What is the object of inquiry? Idealized standards for behavior or idealized standards of societal (dis)approval?
	2. How to distinguish between the content of morality and ideal law? The former concerned with societal disapproval and the latter concerned with coercion.
	3. How to ground an account of wrongness and appropriateness of societal disapproval that does not just reduce to the benefits of societal disapproval? What's the relation between what makes something appropriate to disapprove (as an assessment of historical behavior) and having reason to blame it (as a tool for promoting benefits)? Which is prior?

This deals with the level of analysis between the meaning of normativity and substantive moral theories. It deals with the nature and motivational force of *moral* reasons, i.e. their relation to normativity generally. Moral principles are principles that could be justified to everyone with the aim of finding such principles. In other words, these are principles that could be justified to everyone in group G, where group G is defined as the group interested in principles that can be justified to everyone in G. There are possibly objective standards of correctness for this form of moral judgments, as there is a fixed goal to which there are objectively (in)correct means for satisfying it. The standards of intersubjective justifiability extends beyond morality in the narrow sense of moral rightness and wrongness (e.g. it also sets standards of corrctness for ettiquette and justice). 

Moral discussion concerns what *norms* to accept in society. Particular actions are assessed secondarily based on their conformity with those norms. This allows for some rule-consequential reasoning about the justification of norms, which is not possible when focusing strictly on actions. This is not to say that all justifications must be rule-consequentialist. There might also be deontological justification of certain rules. Note that the concern here is primarily on when certain actions are justified. These are standards for *actions*, as opposed to standards for particular moral emotions, such as blame. The standards for these moral emotions are given below.

### Relevant Considerations

The *justification* of a norm is not based on it being reasonable to any *particular* agent. Thus, a moral system is *completely* unsupported if its justification reduces to "*I* just prefer this system" (e.g. even a system with impartial *content* like utilitarianism must be shown to be reasonable to all parties involved if it is to be *justified*; it is a substantive question whether this can be done). Norms are justified because they can be reasonable from the "moral point of view", i.e. reasonable from the perspective of an agent with the aim of finding principles with intersubjective justifiability. The structural nature of reasons allow for there to be reasons to discount the relevance of other considerations that would serve as reasons in other contexts. When considering the reasons that constitute the moral perspective, there are some constraints on the relevance of certain principles for moral reasoning:

- Personal Reasons: only personal reasons can be reasons to reject a principle. Personal reasons relate to well-being, desires, goals, freedom, etc. They do not include impersonal reasons (e.g. someone who has a concern for the environment) or interpersonal reasons (e.g. someone who has a concern with the interests of others).
- Impartiality: considerations that are indexical can be dismissed. This places an impartiality constraint on moral reasons in three ways: (1) the content - the nature of the duty, (2) the application - who the duty applies to, and (3) the justification of moral norms must not make reference to any proper nouns. E.g. (1) there can be no duties that say "benefit person A".  (2) there can be no duties that vary depending on who someone happens to be (it depends on their circumstances). (3) that A in particular is harmed is not a reason against a principle if all alternative principles would result in a comparable or worse harm for others.
- Degenerate Interests: Interests that either (a) neglect the interests of others or (2) are based upon a desire for the harm of others can be dismissed.
- Responsibility: Harm to those who are responsible is more justifiable tham harm to those who are irresponsible. E.g. if a principle would impose a burden on people by virtue of negligence, poor intentions, etc.
- Aggregation: aggregation in itself doesn't matter. See T.M. Scanlon
- Hard question: how to handle norms that harm some and help others.

Note that principles must be reasonable not to agents in some veil of ignorance, but rather to agents in the actual world. Behind a veil of ignorance, it might be reasonable for all to accept a princple that resulted in a state where well-being was 80 and 60 instead of 100 and 40 (imagine there are two people, A and B). However, in the actual world, if A=100 and B=40, it would be reasonable for A to reject a principle where force was used that resulted in A=60 and B=80, even though this end state is superior.

One question is whether contractualism is committed to solely consequentialist reasons. A consequentualist method of justification first specifies a ranking of ideal states of affairs and then assess actions as right or wrong based on that ranking. Personal reasons would be consequentialist insofar as they are reasons for promoting some end state of affairs. But clearly contractualism is not committed to saying people only have these kinds of reasons. Contractualism allows that people can have reason to hold certain attitudes, attitudes which may be distinct from promoting some good with a positive weight which competes with other goods. However, even if we accept non-consequentialism for personal reasons. We might be consequentialist for moral reasons if we abandon the seperateness of persons. Contractualism is not committed to this either. There is no end state of affairs that is most desirable that we are trying to reach. The procedures matter for their own sake. Principles can be wrong if they permit wrong actions, even if that principle results in a better state of affairs. We do not lose the separateness of persons.
	
These distinctions are similar to consequentialist systems which distinguish between ideal behaviors and decision theories. E.g. Even act-consequentialist say that it is a theory about what conditions makes actions *right* or *wrong*. This is distinct from whether those conditions should be taken into account for any particular agent when deciding what to do (e.g. if it is infeasible for an individual to weigh the consequences in any particular circumstances. 
- So there are three levels of analysis for act-consequentialists: (1) the best state of affairs, (2) right/wrong actions, and (3) decisions about what we ought to do. 
- Rules consequentialism for norms for *behavior* has the following levels of analysis: (1) the best state of affairs, (2) good/bad norms for behavior, (3) right/wrong actions - based on their adherence to the norms, (4) decisions about what we ought to do - probably just refers back to the norms.  
- Rule consequentialism for norms of *moral emotions* has the following levels: (1) the best state of affairs, (2) good/bad norms for moral emotions, (3) right/wrong actions - whether it's the appropriate object of blame given the norms, (4) decisions about whether we ought to blame - probably just refers back to the norm.
- The contractualist analysis: (1) right/wrong actions - just depends on intersubjective justifiability, (2) good/bad norms for moral emotions - depends on whether it promotes/diminishes undesirable behavior.

### Comparison with Veil of Ignorance style characterizations

Two differences:
- Veil of ignorance is concerned with rationality. Contractualism concerned with what is reasonable - i.e. what is reasonable contingent on the aim of finding principles that can be justified to others who are similarly motivated.
- Veil of ignorance strips people of knowledge of their particular features. Contractualism does not?

Possibilities:
- Rationality + awareness of particular features: this can't work because people would be biased towards irrelevant features.
- Rationality + ignorance: Veil of ignorance. This is has bad implications because:
	- It strips away certain irrelevant features. But by only focusing on rationality simpliciter, it still includes some irrelevant features.
	- It only looks at the end distribution of well-being, ignoring the particular individuals who earned/deserve it.
	- If there is a fixed distribution of pleasure/pain to be distributed, it wouldn't matter whether the larger bundles went to people who deserved/earned/virtuous it versus whether it went to the people who don't deserve it or the vicious.
	- Degenerate interests are given just as much weight as everyone else.
	- It doesn't care about whether people are personally responsible for their diminished well-being. E.g. someone who has diminished well-being because they are lazy.
- Reasonableness + awareness: ???
- Reasonableness + ignorance: ???

## Extensions of other systems

### Constructed normative domains

Imagine social games with their own personal rules.
We can use those rules as standards of criticism without rationally criticizing an agent.

Or consider the constructed standards of relationships
- To stand in a certain relation to someone (e.g. friend, lover, partner, cooperator, self-respecting individuals, etc.) has certain expectations for behavior by the parties involved. To the extent that someone violates these expectations, it is appropriate to not extend those behaviors to them, since they would now stand outside of the relationship. 
- This might work for friends and ettiquette (i.e. to the extent that someone doesn't uphold the standards of good friendship, ettiquette, it is appropriate to not treat them as a friend, or with ettiquette). Maybe it also works for aesthetic morality. But maybe this doesn't work for forceful morality (maybe if someone is immoral, we still have moral obligations towards them, i.e. we can exclude them from society, but we cannot kill them).

Other constructed normative domains
- Prudential Rationality: there are no independent standards for bodily movements. But insofar as one is deliberative and reflective and evaluates their own actions, i.e. one is a rational agent, there are standards for behavior.
- Games: there are no independent standards for moving small pieces across a checkered board. But insofar as one is playing chess, there are constituitive standards of the activity that entail standards of correctness.
- Relationships: there are no independent standards of kindness, respect, loyalty, etc. with regard to how to treat others. But insofar as one is being a friend, partner or other normatively-laden relationships, there are constituitive standards of the activity that entail standards of behavior.
- Communication: there are no independent standards of speaking or language. But insofar as one is communicating as a means to transfer ideas to someone else, there are standards of correctness.
- Conversation: To engage in a *discussion* or *conversation* with someone, one implies that they will listen to the other party, not cut them off, etc. even though there may be no independent reason to do thees things. 
- Negotiation: To engage in a *negotiation* implies that people are willing to sacrifice or deviate from their most preferred plan to help satisfy the interests of others. Now, one might have no reason to do these things (i.e. if they had full power), but then they wouldn't be negotiating. They would be bad negotiators. 
- Morality: there are no independent standards of behavior. But insofar as one is engaged in moral argument as a procedure for finding *impartial reasons*, or reasons that all can accept, there are constituitive standards of correctness.

Communicative constraints regarding reasons for belief and/or conversations/arguments generally.
- Burden of proof depends on the speaker who asserts a claim, not on the mere content of the proposition. That someone asserts a claim has a burden of proof cannot be entailed from the content of the propositions expressed. When Mike says "God Exists", it follows that he has reason to follow the rule, even though this reason is not trivially included in the rational extension of his motivational set for beliefs.
- Even though an anecdotal experience might provide one with a personal reason for belief, it would not provide everyone with a reason for belief (e.g. a scientific reason).
- How to know what the standards are? We check what would happen if everyone accepted it. If no one accepted a burden of proof, the central goal of argument would be defeated.
- Don't interrupt others, cut them off, etc. Listen to them.

### Intersubjective Justification

Many normative domains are concerned with what can be justified to different agents, not any individual agent. E.g.

- Personal experience / intuition / presuppositions might provide an individual agent with reasons for believe. But they cannot independently provide anyone else with a reason for belief (unless the other agent believes the other person's experience/intuition/etc. are accurate). Science is a system that searches for intersubjective justification for beliefs. Thus, such instances are not scientifically justified.
- That a principle benefits A at the expense of everyone else might be a reason for A to adopt the principle, but that is no reason for anyone else to accept it (e.g. a principle that said everyone has a moral obligation to maximize my pleasure). Thus, such a principle cannot be morally justified.

### Golden Rule

- Golden Rule: Treat others in the way you would want to be treated.
	- Upshot: don't kill if you wouldn't want to be killed. 
	- Problem 1: what if I would have different desires than the recipients? E.g. a masochist wouldn't mind being attacked.
	- Problem 2: what if I would have desires that people treat me unreasonably? E.g. I would want others to always benefit me at the expense of others. 
- Moral Emotions: Treat others in the way that you would not blame someone for treating you
	Upshot: You can treat others in a way that you would dislike, so long as you wouldn't blame them for it.
- Rationalized: Treat others not based on counterfactual desires, but on our counterfactual reasons for blame.
	- Upshot: don't kill if they don't have reason to want to be killed (e.g. even if they're in an intoxicated state where they want to be killed).
	- Doesn't solve either problem. Just idealizes desires/blame.
- Inversed: Treat others in the way they want you to treat them or would not blame you for.
	- Upshot: don't attack if the person doesn't want to be attacked.
	- Solves Problem 1.
- Abstracted: Treat others in accord with principles any rational agent would endorse, independent of their actual circumstances, desires, interests, etc. (i.e. veil of ignorance), if the agent knew he had a chance of any combination of circumstances or desires.
	- Upshot: don't kill it would be forbidden by any principles that no one could rationally reject.
	- Solves Problem 1 and Problem 2.
	- Reasons from Ignorant self-interest =/= reasons from morality.
- Moralized: Treat others in accord with principles no rational agent could rationally reject, contingent upon that agent having a goal of finding such principles that others couldn't rationally reject.

### Negotiation

- Normal negotions: 
	- Two parties are interested in trading goods. 
	- Each party has a ranked set of alternative systems of trade they would be willing to accept.
	- e.g. one party is willing to pay no more than $100 for some good X. The other is willing to sell X for no less than $50.
	- A good negotiation is one where X is sold for somewhere between $50 and $100.
	- The exact number to make the sell is indeterminate.
	- Agents may not have *reasons* to be good negotiators, i.e. if one party could coerce the other, that might be in their best interests.
- Morality features the following augmentations
	- Concerns alternative systems of principles to regulate society; not systems of trade.
	- Impartiality: Abstracts from any particular irrelevant circumstances, i.e. wealth, power, fame, race, etc. 

## Sentimentalism

Even if one rejects the contractualist analysis, we can still give a general theory of truth for all moral systems, even those which do not adhere to the contractualist characterization given above. This can be done by treating moral judgments like aesthetic judgments. 

This concerns the attitudes or emotions (e.g. disgust, blame, shame, resentment, guilt, anger, etc.) involved in moral judgments and makes them akin to aesthetic attitudes. These would be any moral judgments we hold regarding a certain behavior despite not judging that they have any intersubjective justifiability. For example, we might mantain a disgust reaction to, e.g., a man has sex with a corpse, even if it isn't unjustifiable given the aim of intersubjective justifiability (e.g. if we somehow know that this doesn't affect the man's relationship with others, that allowing this in society will not have negative effects, etc.). This is not to say that these judgments do not regularly figure in moral arguments. Sometimes they do. But when they do figure in, the argument merely concerns systemezing whatever morally optional sentiments we have ("optional" in the sense of not being necessary from the perspective of intersubjective justifiability). 

There are possibly two ways of establishing truth for these kinds of moral judgments: (1) dispositionalism and (2) rational sentimentalism. Dispositionalism focuses on the refined attitudes that an agent would have under certain idealized conditions, e.g. full information, full deliberation/reflection, full experienced, under a sound state of mind, etc. Rational sentimentalism focuses on what is *constituitive* of moral emotions to determine the appropriate object of said emotions. 

Dispositionalism states the following: one has reason to feel attitude R with regard to X if and only if X is such as to elicit R in circumstances C. The circumstances C can be given several formulations such as, e.g., normal circumstances. This is similar to the truth conditions for when an entity is a certain color; i.e. X is red if and only if X is such as to elicit the perception of redness in normal humans under normal circumstances. However, moral judgments must be based on the attitudes in *idealized* circumstance rather than "normal" circumstances because of the following problems with using "normal" circumstances: (1) It cannot be used to criticize conventional or one's present - assuming they are normal 0 attitudes, (2) It doesn't explain why one's evaluative judgments influence their attitudes, whereas one's judgments about color concepts don't influence their color perceptions, and (3) It doesn't explain why moral truth can be discovered a priori under reflection, whereas truth about color concepts cannot.

Thus, moral truth depends on certain agent's counterfactual attitudes under idealized conditions. Truth could either be specific to an individual's idealized attitudes or the idealized attitudes of normal agents. If the latter, this may allow for some rigidifcation which creates a universal standard for truth common to all agents in the actual world. Regardless, because correctness depends on contingent psychological sensibilities, correctness is not mind-dependent in a way that allows for robust objectivity. This is similar to aesthetic judgments in general (think ettiquette) and perhaps secondary properties. Truth would be based on a refinement of one's attitudes with experience, imagination, deliberation, etc. It doesn't seem that there would standards for moral correctness independent of moral assumptions. Thus, reflective equilibrium seems to be the dominant method for seeking moral truth.

Rational sentimentalism states: one has reason to feel R with regard to X if and only if X has the properties ascribed by R. For example, to fear X involves some sort of perception that it is dangerous. This is constitutive of fear, and there is clear evolutionary reason to develop such a psychological faculty. One has reason, then, to fear X if and only if X is dangerous. Moral emotions, e.g. blame, resentment, guilty, etc., are emotions like that. Unforunately, there may in fact be no constituitive standards of attitudes such as anger, resentment, etc. apart from moral judgments that something is morally wrong. That is, it may not be possible to describe the constituitive features of moral attitudes using non-moral terms.

## Motivation

Both forms of morality (sentimentalism and contractualism) explains why agents have reason to endorse certain moral principles. But neither explains why an agent has reason to abide by the duties prescribed by the moral principles. An agent would have such a reason insofar as they either (1) have natural sympathy with the welfare of others (meaning the reasons that others hold would be reasons that they also hold), or (2) judge that their behavior should be consistent with the norms they endorse.

## Methodology/Objectivity

Morality is rationally optional, but this doesn't diminish its authority:

- Contingent on our interests:
	- Nearly universal: most people interested in impartial/social reasons, having a system of norms regulating blame.
	- Emphasizes the negotiation aspect: people cant just have independent that they claim are objective with no consideration of other person's reasons.
	- Other analogues: Scientific reasons are impartial/social reasons for belief (and thus evidentially optional), but that doesn't diminish its authority.
- Not externally necessarily justifiable, like all normative domains
	- Deduction/induction/abduction/evidence/science cannot be reasonable to someone who doesn't care for those forms of reasoning
- Not ontologically independent
	- Who cares about ontology
- No shared inputs
	- With science, we share the same perceptions/observations?
	- With morality, there is widespread disagreement. Might there be a general principle that all must agree to?

There are no judgment-independent standards of moral truth, but that's a good thing:

- People cannot just come with arbitrary principles and claim them as the moral virtues.
- They are forced to take into account the interests of others.
- They are forced to make their demands reasonable to others. 
- Moral reasoning is a process of negotiation rather than of discovery.

Objectivity: imagine people have widely divergent beliefs. Objectivity is possible when there happens to be an intersection in the beliefs held by these people. More specifically, it's possible when there is an intersection in what these people judge to be solid methods for establishing truth. E.g. think of science. People might have widely different beliefs. But everyone agrees that one's beliefs should cohere with their other beliefs, and we happen to be beings with sensory inputs that automatically impress beliefs within us (which means our beliefs must cohere with our sensory inputs), and our sensory inputs happen to converge a lot of the time. There are other features we happen to agree with as well (or can be shown to agree with), e.g. repeated observations from different independent sources/experiments provide more evidence for a hypothesis than one-offs, etc. It is this convergence that grounds the objectivity of science. I.e. two people might disagree over whether theory X or Y is true, but they can agree on method for determining whether X or Y is true, e.g. by appealing to observation, abduction, induction. Without this, objectivity is impossible. Even if we disagree on method, we can ideally point to a meta-principle that we both agree with (or can be shown to agree with) that settles which method is valid (e.g. reflective equilibrium). 

Might there be a similar methodology for practical rationality or ethics? There is no reliable, impartial, etc. methodology for determining what a person has reason to do, so there is clearly no such methodology for ethics. So how can we determine the content of moral principles? (1) We determine the content of practical rationality (what is good for individuals, or what individuals have reason to do) and (2) We determine how morality relates to practical rationality. Once we agree on what makes an ordinary person's life go well and determine how wellness relates to morality, we can determine moral content. We must determine what is good by appealing to intuitions we already agree with. If someone completely disagrees with what makes one's life go good, then surely they will never agree with any moral principles. However, appealing to shared intuitions about goodness is useful: people are more likely to agree on what makes one's life go best than they are to agree with what is morally right or wrong. E.g. at a minimum, people must agree that one's life goes best when provided with positive liberty to do what they want. Note that we don't need agreement in our moral views; we need agreement in what constitutes a good life. 

Disagreement
- There is widespread moral disagreement. This is explained by intuitionist by the fact that we have different beliefs, different circumstances (circumstances which justify different moral positions, e.g. infanticide in food-sparse areas) or different levels of rationality. The basic moral perceptions, it is claimed are actually identical.
- I agree that much disagreement can be reduced to these features and not disagerements in basic moral values (I don't use perceptions). However, there is no reason to believe that all fully informed, fully rational beings would agree about that is morally appropriate in different circumstances.
- I can agree that there would be near-unanimous agreement about what kinds of considerations are relevant and irrelevant (see above: well-being, liberty, autonomy, etc.) in fully informed, fully rational humans. 
- However, there will be widespread disagreement on what to do when these considerations conflict (e.g. harm one person to help another). In developed socities, for example, there is widespread disagremeent about using force against innocent to assist the disadvantaged. We need a procedure to determine which considerations are to trump others in which circumstances.

Some notes:
- Reason is purely procedural. It is concerned with amelierating *conflicts* and striving for *coherence* of our aims.
- Badness only occurs when one's idealized aims are suppressed in some way.
- Moral wrongness occurs only if it results in someone's life being worse in some way.
	- If action X resulted in no one being worse-off or everyone better-off, then it cannot be morally wrong.
	- This is not to make a claim about consequentialism or deontology. One might say its morally wrong to make anyone worse off.
